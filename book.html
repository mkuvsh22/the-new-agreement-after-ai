<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Новый договор | The New Agreement</title>
    <!-- SEO & Open Graph -->
<meta name="description" content="A reflection on cinema after AI — where vertical screens, neural images, and Deleuzian theory collide.">
<meta property="og:title" content="The New Agreement — Cinema and the Viewer After AI">
<meta property="og:description" content="A reflection on cinema after AI — where vertical screens, neural images, and Deleuzian theory collide.">
<meta property="og:type" content="website">
<meta property="og:url" content="https://твоя-ссылка.github.io/book.html">
<meta property="og:image" content="https://твоя-ссылка.github.io/45.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:title" content="The New Agreement — Cinema and the Viewer After AI">
<meta name="twitter:description" content="A reflection on cinema after AI — where vertical screens, neural images, and Deleuzian theory collide.">
<meta name="twitter:image" content="https://твоя-ссылка.github.io/45.png">

    <link rel="icon" type="image/png" href="45.png">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: Poppins, sans-serif; /* ШРИФТ - меняйте здесь */
            background-color: #ffffff;
            color: #000000;
            line-height: 1.6;
        }
        /* Стили для библиографии */
#bibliography .chapter-content {
    max-width: 800px;
    margin: 0 auto;       /* центрируем блок */
    padding: 0 20px;      /* внутренние отступы слева и справа */
}

#bibliography .chapter-content ul {
    padding-left: 1.5em;  /* стандартный отступ списка */
    list-style-position: inside; /* чтобы маркеры были внутри */
}

#bibliography .chapter-content li {
    margin-bottom: 0.5em; /* небольшой промежуток между элементами */
}

#bibliography .chapter-content h3 {
    margin-top: 1em;
    margin-bottom: 0.5em;
}


        /* ПЕРВЫЙ ЭКРАН - ОГЛАВЛЕНИЕ И ОБЛОЖКА */
        .hero-section {
            display: flex;
            height: 100vh;
            position: relative;
        }

        /* ЛЕВАЯ ЧАСТЬ - ОГЛАВЛЕНИЕ */
        .table-of-contents {
            width: 50%; /* ШИРИНА ОГЛАВЛЕНИЯ - меняйте здесь */
            padding: 20px 30px;
            background-color: #f5f5f5; /* ЦВЕТ ФОНА ОГЛАВЛЕНИЯ - меняйте здесь */
            overflow-y: auto;
        }

        .toc-title {
            font-size: 21px; /* РАЗМЕР ЗАГОЛОВКА КНИГИ - меняйте здесь */
            font-weight: bold;
            margin-bottom: 10px;
            color: #000000; /* ЦВЕТ ЗАГОЛОВКА - меняйте здесь */
        }

        .toc-subtitle {
            font-size: 20px; /* РАЗМЕР ПОДЗАГОЛОВКА - меняйте здесь */
            margin-bottom: 40px;
            color: #666666; /* ЦВЕТ ПОДЗАГОЛОВКА - меняйте здесь */
        }

        .toc-list {
            list-style: none;
        }

        .toc-item {
            margin-bottom: 15px;
        }

        .toc-link {
            font-size: 16px; /* РАЗМЕР ШРИФТА ПУНКТОВ - меняйте здесь */
            color: #587291; /* ЦВЕТ ССЫЛОК - меняйте здесь */
            text-decoration: none;
            transition: color 0.3s;
            display: block;
        }

        .toc-link:hover {
            color: #31081f; /* ЦВЕТ ССЫЛОК ПРИ НАВЕДЕНИИ - меняйте здесь */
        }

        /* ПРАВАЯ ЧАСТЬ - ОБЛОЖКА */
        .cover-section {
            width: 50%; /* ШИРИНА ОБЛОЖКИ - меняйте здесь */
            display: flex;
            align-items: center;
            justify-content: center;
            background-color: #ffffff; /* ЦВЕТ ФОНА ПОД ОБЛОЖКОЙ - меняйте здесь */
            padding: 40px;
        }

        .cover-image {
            width: 120%;
            height: 120%;
            object-fit: contain;
            box-shadow: none; /* ТЕНЬ ОБЛОЖКИ - меняйте здесь */
        }

        /* КОНТЕНТ ГЛАВ */
        .chapters-content {
            max-width: 800px;
            margin: 0 auto;
            padding: 80px 40px;
        }

        .chapter {
            margin-bottom: 80px;
            padding-top: 40px;
        }

        .chapter-title {
            font-size: 28px; /* РАЗМЕР ЗАГОЛОВКА ГЛАВЫ - меняйте здесь */
            font-weight: bold;
            margin-bottom: 30px;
            color: #000000; /* ЦВЕТ ЗАГОЛОВКА ГЛАВЫ - меняйте здесь */
        }

        .chapter-content {
            font-size: 16px; /* РАЗМЕР ТЕКСТА ГЛАВЫ - меняйте здесь */
            line-height: 1.8; /* МЕЖСТРОЧНЫЙ ИНТЕРВАЛ - меняйте здесь */
            color: #333333; /* ЦВЕТ ТЕКСТА ГЛАВЫ - меняйте здесь */
        }

        .chapter-content p {
            margin-bottom: 20px;
        }

        /* ССЫЛКА НАЗАД */
        .back-link {
            position: fixed;
            top: 20px;
            right: 20px;
            font-size: 16px; /* РАЗМЕР ШРИФТА ССЫЛКИ HOME - меняйте здесь */
            color: #587291; /* ЦВЕТ ССЫЛКИ HOME - меняйте здесь */
            text-decoration: none;
            transition: color 0.3s;
            z-index: 100;
        }

        .back-link:hover {
            color: #31081f; /* ЦВЕТ ССЫЛКИ HOME ПРИ НАВЕДЕНИИ - меняйте здесь */
        }
        .note {
        margin: 30px 0 30px auto;
        padding: 15px 20px;
        max-width: 70%; /* Ширина примечания */
        border-left: 3px solid #999999; /* Черта слева - цвет меняйте здесь */
        background-color: #f9f9f9; /* Фон примечания - меняйте здесь */
        font-size: 14px; /* Размер шрифта - меняйте здесь */
        font-style: italic;
        color: #666666; /* Цвет текста - меняйте здесь */
        text-align: left;
        }

        /* Адаптивность */
        @media (max-width: 768px) {
            .hero-section {
                flex-direction: column;
            }

            .table-of-contents,
            .cover-section {
                width: 100%;
            }

            .table-of-contents {
                padding: 40px 20px;
            }

            .cover-section {
                height: 50vh;
            }

            .chapters-content {
                padding: 40px 20px;
            }
        }
    </style>
</head>
<body>
    <!-- Ссылка назад на главную -->
    <a href="index.html" class="back-link">home</a>

    <!-- ПЕРВЫЙ ЭКРАН -->
    <div class="hero-section">
        <!-- ЛЕВАЯ ЧАСТЬ - ОГЛАВЛЕНИЕ -->
        <div class="table-of-contents">
            
            <h1 class="toc-title" style="margin-bottom: 40px;">The New Agreement. Cinema and the Viewer After AI</h1>
            
            <ul class="toc-list">
                <li class="toc-item"><a href="#preface" class="toc-link"><strong>Preface. Autobiography of an Eye</strong></a></li>
                <li class="toc-item"><a href="#chapter1" class="toc-link"><strong>Chapter 1. How I Became a Token</strong></a></li>
                <li class="toc-item"><a href="#chapter2" class="toc-link"><strong>Chapter 2. Loss of Control. How Generative AI Turned Us into Surrealist</strong></a></li>
                <li class="toc-item"><a href="#chapter3" class="toc-link"><strong>Chapter 3. Cyborg, Monster, Homo. How Cinema Turned Us into Posthumanists</strong></a></li>
                <li class="toc-item"><a href="#chapter4" class="toc-link"><strong>Chapter 4. The Face as Manifesto. Post-Cinematic Celebrities in the "Uncanny Valley"</strong></a></li>
                <li class="toc-item"><a href="#chapter5" class="toc-link"><strong>Chapter 5. Keep Watching. How Big Data, Neural Networks, and Diffuse Responsibility Are Changing Cinema</strong></a></li>
                <li class="toc-item"><a href="#chapter6" class="toc-link"><strong>Chapter 6. Anti-Deleuze: Classical Film Theory Under the Pressure of AI (Without Losing Its Explanatory Power)</strong></a></li>
                <li class="toc-item"><a href="#chapter7" class="toc-link"><strong>Chapter 7. Yes, I Love You! How We Got Used to Vertical Images</strong></a></li>
                <li class="toc-item"><a href="#chapter8" class="toc-link"><strong>Chapter 8. Transit Zone. From Digital Photography to In-Game Documentary</strong></a></li>
                <li class="toc-item"><a href="#chapter9" class="toc-link"><strong>Chapter 9. The Eye of God. Jon Rafman as Prophet of the New Visuality</strong></a></li>
                <li class="toc-item"><a href="#acknowledgments" class="toc-link"><strong>Disclaimer | Acknowledgments</strong></a></li>
                <li class="toc-item"><a href="#bibliography" class="toc-link"><strong>Bibliography</strong></a></li>
            </ul>
        </div>

        <!-- ПРАВАЯ ЧАСТЬ - ОБЛОЖКА -->
        <div class="cover-section">
            <img src="cover11.png" alt="The New Agreement - Book Cover" class="cover-image">
        </div>
    </div>

    <!-- КОНТЕНТ ГЛАВ -->
    <div class="chapters-content">
    <div class="translation-note" style="font-size: 13px; color: #cc0000;">
    <span style="background-color: #FF0000; padding: 2px 4px;"><span style="color: #FFFFFF;">This is an authorized machine translation of <i>The New Agreement. Cinema and the Viewer After AI</i> by Maria Kuvshinova, published in Russian by Ivan Limbach Publishing House (Saint Petersburg) in 2025.</span></span>
</div>
        <!-- PREFACE. Autobiography of an Eye -->
        <section id="preface" class="chapter">
            <h2 class="chapter-title">Preface. Autobiography of an Eye</h2>
            <div class="chapter-content">
                <p>This book is the autobiography of a film critic. A media creature that indiscriminately consumes visual content from any source. But I tried to make it interesting not only for fans of the autofiction genre.</p>
    
    <p>I have been writing about cinema since the mid-1990s, and by the late 2000s it became increasingly clear that cinema was undergoing a transformation. The vast visual ocean of the 20th century began shrinking into an island in the 21st, reshaped by digital technologies and competing media. These observations led to my book <i> Cinema as Visual Code </i> and the conference <i>History of Cinema as Media Archaeology</i>, which I organized with colleagues from <i>Seance</i> magazine at the New Stage of the Alexandrinsky Theatre in 2015.</p>
    
    <p>Film criticism is both an isolated and remarkably accessible profession. You don't need theoretical training to do the fieldwork — you just need to watch films and write about them. My book <i>Cinema as Visual Code</i>, affectionately nicknamed "visual cat" by friends, was essentially a collection of guesses and hypotheses with little theoretical grounding. Sitting in theaters or in front of my computer, I was discovering and describing in real time what theorists had already articulated years before. Back in 2015, my colleague Alexandra Akhmadshina suggested inviting the renowned film and media scholar Thomas Elsaesser to our conference — and to our surprise, he accepted. His visit ti St. Petersburg had a lasting impact: later <i>Seance </i> published a Russian edition of his book <i>Film Theory: An Introduction through the Senses </i>(with Malte Hagener). Elsaesser and Hagener, I found, were wrestling with the very questions that preoccupied me — how cinema was being reshaped by digital technology, how physical media (from celluloid to DVD) influenced both film culture and filmmaking itself, the bodily experience of watching movies. Maybe I should stop fretting about <i>Visual Code's</i> theoretical gaps and simply be glad my instincts aligned with Elsaesser's thinking (rest assured, I've since learned proper citation practices — I had to actively resist turning this book into a patchwork of quotations)</p>
    
    <p>In a decade numerous books on media theory and film studies have appeared in Russian, foreign scholarship has become more accessible, and digital transformation has accelerated — culminating in 2022 with the arrival of publicly available AI models that generate text, images, and video. This new tool — another dominant force reshaping our world, and the subject of fierce debate — is fundamentally altering what I call the agreement between screen and viewer: that set of often unspoken conventions governing our relationship with moving images.</p>
    
    <p>Each new stage in the history of visual media entails a renegotiation of its tacit contract with the viewer — a process that unfolds gradually and involves broad participation. (“Do you keep the copies of old agreements?” a user once asked me on what is now called X.) Artists once protested against photography; the first audiences of the Lumière brothers, as legend has it, fled in panic from the oncoming train. Today, generative images and videos — dismissively labeled “AI slop” — provoke hatred in some, curiosity in others, and for yet another group appear strangely authentic, as if they were direct imprints of reality.</p>
    
    <p>Most of the generated viral content circulating on social media in the mid-2020s rework familiar images and faces: morphings that age or rejuvenate famous actors, musicians, and politicians; world leaders parading down the runway, sometimes as toddlers, sometimes as superheroes accompanied by giant animals symbolizing their nations; Viktor Tsoi and Sergei Bodrov resting between takes on the set of a celestial <i>Brother 3</i>. Even the “Italian brainrot” meme (“bombardillo crocodillo”), which seemed to have emerged without human intervention, evoked — at least in the Russian-language internet — the worlds of children’s writers Eduard Uspensky and Korney Chukovsky. Thus, retrotopic consciousness finds expression through the most advanced and postmodern of all technologies: generative models that assemble new images and texts out of everything that was once created and later digitized.</p>
    
    <p>The digital future of nostalgia, foreseen by Svetlana Boym in her timeless book, has materialized as the visualization of collective memory — as the activation of culture’s “potential space.” It takes shape in visual simulations of a past that never existed: Soviet childhood imagined through the lens of Wes Anderson; socialist modernist architecture transplanted to New York; pioneers rehearsing their lines yet speaking unpredictable words into the camera.</p>
    
    <p>However, in this new type of moving images, something else is added to familiar images and emotions — the otherworldly effect of the "uncanny valley," the inhuman plasticity of human beings, sudden visual metastases. Gilles Deleuze once said that "the brain is the screen." Many theorists, whose conclusions are confirmed by modern neuroscience, agree that with each encounter with a new form of media, a special kind of conscious experience arises. Like <i>Visual Code</i>, this book is the result of my observations of the transformation of the visual and our relationship to it. An attempt to look into the "black box" of technology and record the parameters of the new contract between screen and viewer that is emerging before our eyes.</p>
    <p></p>
    <p> <i>Saint Petersburg, August 2025</i></p>
    
            </div>
        </section>

        <!-- CHAPTER 1 -->
        <section id="chapter1" class="chapter">
            <h2 class="chapter-title">Chapter 1. How I Became a Token</h2>
            <div class="chapter-content">
                <p>A few months after the Lumière brothers held their first public screening on the Boulevard des Capucines, they dispatched cameramen to every corner of the world to <I>bring the world to itself</I>: to capture fragments of daily life — streets and landscapes, parades and exhibitions, the faces of strangers passing by. What was born in 1895 was not yet the art of cinema, but the business of it. The ever-growing catalogue of moving images was part of the Lumières’ commercial vision: they sold cameras, distributed films, and in doing so built the first global network of motion pictures. Their project reached some thirty countries, from Austria-Hungary to Mexico, from Australia to Indochina. Among those sent abroad was the journalist Camille Cerf, who travelled to Russia to film the coronation of Nicholas II.</p>
                <p>Another operator trained in Lyon became a minor character in Aleksei Balabanov’s short film <i>Trofim</i> — part of an anthology by young Russian directors made to commemorate the centenary of cinema. A peasant, having killed his brother in a fit of jealousy, flees to St. Petersburg, where he sees a Frenchman with a movie camera — a genuine wooden Lumière model found in the storerooms of Lenfilm Studio. Trofim hesitantly steps into the frame; ninety years later, the same man — arrested in a brothel and, most likely, executed — is cut out of the archival footage by a director working on a Russian–German project titled <i>The Railway Stations of St. Petersburg</i>. A man once dissolved into nothingness briefly resurfaces a century later, only to vanish again at the will of a dictator–director — this time for good.</p>
                <p>Seen from today’s perspective, the Lumières were doing more than recording reality — they were building what we would now call the first structured dataset of moving images. Balabanov’s fictional Trofim, like any other passerby accidentally captured by the camera, was a token — a minimally meaningful unit of information within a larger dataset. His fate mirrors the discrete and fragmentary nature of historical memory: he emerges from oblivion for a brief instant, only to disappear again, much like a token that can be activated, deactivated, and deleted within a system. In Balabanov’s film, the director — played by Aleksei German — removes the character because he “spoils the frame.” The deletion of tokens in datasets serves a similar purpose: noise reduction, the elimination of errors that distort the model. A quest to perfect the system and its outputs.</p>
                <p></p>
                <p>***</p>
                <p></p>
                <p>The Lumières, intent on expanding their business, could not have imagined that their commercial footage would one day be reinterpreted as data — or even become part of the very datasets used to train today’s video-generating models such as Runway, Sora, Veo 3, or Luma. In a way, the brothers fell into the same trap that now ensnares us all. The current phase of AI development is inseparable from social media — from the endless terabytes of content we have produced ourselves, freely and compulsively, and on which contemporary language and vision models have been trained. These models now speak in human voices and dream in human images. The speed and inevitability with which our emotions, our connections, our desire for belonging and self-expression — all captured within the architectures of social media — have turned us into a resource for what the Greek economist Yanis Varoufakis calls "cloud capital" is only now being critically examined, and for many, rightly, it evokes a justified fear.</p>
                <p>But I’d like to leave the realm of social and political abstraction and return to human scale — to try to describe what I feel today: a small token, dissolved in the endless universe of big data. What does it mean to give the products of my labor not only to corporations, but also to a form of life that, as some believe, may one day replace humanity itself?</p>
                <p>We’ve been prepared by pop culture and cinema for decades to meet intelligent machines and artificial minds (I’ll return to this in <a href="#chapter3">Chapter 3</a>). But since one well-known Greek author has already been mentioned, it seems only fair to bring up another. In 2011, the filmmaker Yorgos Lanthimos — not yet shooting in English and still far from the global art-house spotlight — premiered his third feature, <i>Alps</i>, at the Venice Film Festival. At the heart of the story is a clandestine group that doubles as a small business. Its members call themselves after Alpine peaks — their leader goes by Mont Blanc. They gather details about the recently deceased and, for a modest fee, step into their place. They go to the movies with grieving relatives, repeat familiar lines, reenact gestures once belonging to someone else — all in an attempt, however fleeting, to fill the void that remains.</p>
                <p>His previous <i>Dogtooth</i> (2009), the one that made him a cult figure among intellectuals, explored how family and society shape our perception of reality: parents keep their adult children locked up, feeding them distorted explanations of the most ordinary things (“a cat is a dangerous beast”). <i>Alps</i> went further, questioning what it even means to be human. Back then, social media was still in its infancy — Facebook (now owned by Meta, banned in Russia) had only recently opened to the public in 2006. And yet, despite never mentioning digital technology, Lanthimos’s early film already saw human beings as bundles of data — collections of routines and anxieties that could be mapped, coded, and transferred onto another vessel. The replica, disturbingly, functioned no worse than the original. I remember being struck by that realization while watching Alps — years before the rise of AI — as if the film had triggered an early reflection on my own gradual transformation into a set of data, into a token. Lanthimos agreed with my interpretation when I interviewed him, though he didn’t elaborate. Looking back, I think <i>Alps</i>, more than any Hollywood sci-fi about machine uprisings, prepared me for the encounter with artificial intelligence.</p>
                <p>There is something profoundly unsettling in the sense that our subjectivity is being reduced to the status of a mere token. Like Trofim, the hapless character in Balabanov’s film, we step hesitantly into the frame of history, unaware that we have already become part of someone else’s dataset. For now, it is mostly artists and illustrators who seem most alarmed by this shift — those who resist having their work absorbed into the training sets of models that now generate images, often for commercial use. Yet the process seems impossible to halt. Descending an escalator in the St. Petersburg metro, I suddenly realized that most of the faces in the glowing advertisements around me no longer belonged to real people. Digital ghosts stared back — infinitely cheaper to produce than a studio photoshoot. I doubt that most passengers even notice the difference.</p>
                 <p></p>
                <p>***</p>
                <p></p>
                <p>In 2022, after our first summer with Midjourney, Spawning.ai — a company which builds tools to protect creative rights in the age of AI — launched Have <i>I Been Trained?</i>, an online platform that helps artists, photographers, and other creators find out whether their work has been used to train generative models. By searching with text or image, users can peer into LAION-5B — a vast open dataset of over five billion image–text pairs — to see what kind of material underlies AI systems and to identify cases of copyright infringement or unconsented use. The platform also gives creators the option to request that their works be excluded from future datasets. However vast it may be, LAION-5B is still just a drop in the digital ocean — a drop through which the shape of our new reality begins to shimmer into view.</p>
                <p>Using Have <i>I Been Trained?</i>, I discovered not only photographs of my own face taken from various websites and social networks, but also an image I had captured of the Filipino filmmaker Brillante “Dante” Mendoza. I took it with a Sony digital camera during the closing press conference at the 2009 Cannes Film Festival. The low-resolution photo still appears on his Wikipedia page in several languages. Years ago, an editor for the online encyclopedia came across it on my Flickr account, and asked me to release it under a Creative Commons license, which permits free distribution of content. At the time, no one could have imagined that this photograph might one day be used to train a neural network.</p>
                <p>The irony of digital reality is that my entire photo archive was lost on a failing hard drive, Flickr at some point deleted the accounts of its free users, and Wikipedia turned out to be the accidental means of preserving not only this image but also my memories of jostling among other journalists around the table in the press hall at the Le Palais des Festivals, where the newest laureates were brought in one by one from the Lumière theater.</p>
                <p>Anyone who works  with neural network generations knows that generating images, text, or code happens inside a “black box”. Modern AI systems, especially large language models (LLMs), are extremely complex mathematical constructions, so even their creators do not fully understand how they function. “We built it, we trained it, but we don’t know what it doing,” Sam Bowman, a university researcher and scientist at Anthropic — the company behind Claude, a competitor to ChatGPT — told <i>Vox</i> in an interview.</p>
                <p>The first Filipino filmmaker to win a directing prize at Cannes in 2009, Mendoza holds an honored place in the festival hierarchy, yet he is not nearly as well known globally as, for example, Isabelle Huppert, who once starred in one of his films. That is precisely why the Wikipedia editor reached out to me: he couldn’t find any other suitable, copyright-free photographs of the director. I also clearly remember deliberately uploading this image to my Flickr account, aware that it might be in demand exactly because Mendoza, unlike Hollywood or European film stars, was rarely photographed. Using <i>Have I Been Trained?</i>, the LAION-5B dataset contains only 533 images tagged with the name Brillante “Dante” Mendoza. About 230 of these, nearly half, turn out to be useless “noise,” such as posters or film stills, essentially visual clutter, while five are variations of the very same photograph I took.</p>

                <div style="text-align: center; margin: 20px 0;">
                <img src="dante.png" alt="Brillante Mendoza at the 2009 Cannes Film Festival" style="max-width:100%;">
                <div style="font-style: italic; font-size: 0.85em; margin-top: 5px;">
                Ill. 1. My photograph from Wikipedia with the caption "Brillante Mendoza at the 2009 Cannes Film Festival" (left) and the results of generations from a neural network service based on Stable Diffusion, trained on LAION-5B.
               </div>
               </div>
                <p>By examining these fragments of the dataset, one can lift a corner of the “black box” and venture the assumption that my photograph played some role in what neural networks trained on LAION-5B are now able to recognize and reproduce — albeit only approximately — the image of filmmaker Brillante Mendoza. Could it be that it was from my photo that Stable Diffusion, the model underpinning many image-generation services, learned that Mendoza wore a bow tie at the 2009 Cannes closing ceremony? There is no reliable way to confirm it. <p>
                </p>
                <div class="note">
    To be fair, when prompted with “Brillante Mendoza at the 2009 Cannes Film Festival,” the model occasionally did produce images of him wearing a tie.
</div>

                <p>But if it really did come from my photograph, isn’t there a kind of immortality in that — a different form from those described by Milan Kundera in the novel of the same name, where “small immortality” lives in the memory of loved ones and “great immortality” in the thoughts of strangers? This would be an anonymous, digital immortality brought to life in practice. I want to believe so.</p>
                 <p></p>
                <p>***</p>
                <p></p>

                <p>In 1936, the German engineer Konrad Zuse began building a machine in his parents’ apartment in Berlin — a project he would continue throughout the war — that would later be recognized as the first digital computer. Programming was done using punched tape, which was in fact repurposed 35-millimeter film found in trash bins near a film studio. One strip of the film shows binary code punched directly onto a mundane scene with two actors in a small room. Media theorist Lev Manovich recounts Zuse’s story in his essay <i>How Media Became ‘New’</i>, and notes: “Whatever meaning and emotion was contained in this movie scene has been wiped out by its new function as a data carrier. <…> In a technological remake of the Oedipal complex, a son murders his father. The iconic code of cinema is discarded in favor of the more efficient binary one. Cinema becomes a slave to the computer.”</p>
                <p>This early instance of the unusual merging of film imagery and programming code, occurring long before the digitization of the filmmaking process itself, became for me another bridge connecting cinema to the new digital technologies I studied during my Art & Science master’s program at ITMO University.</p>
                <p>At the end of the first year of this program, which promises to turn anyone into an artist in just a couple of years, we were required to present our work at an exhibition in the AIR space, located in one of ITMO’s buildings on Vasilievsky Island. Although I left the master’s program shortly before graduation, I did manage to obtain a certificate in continuing education as a Data Analyst, along with a basic understanding of the Python programming language, and I was eager to put these unexpected skills to use in my course project.</p>
                <p>When I started my project, I was already aware of the ethical nightmare posed by datasets — digital dumps whose environmental impact has never been addressed. The literature is full of discussions on these issues, especially in the excellent book <i>The Atlas of AI</i> by Kate Crawford. Using images and texts from the internet without the consent of rights holders, compiling datasets opaquely, drawing on police databases to train facial recognition algorithms that distance people from their own bodies and life stories, and transferring human biases into AI — all of these raise serious ethical concerns.</p>
                <div class="note">
    In 2020, MIT removed the Tiny Images dataset, which was popular among developers and contained 80 million images from Google, because many of the pictures were problematic and the accompanying labels inappropriate. For example, women were described using derogatory terms, and body parts were labeled with offensive slang.
</div>

                <p>Working with my own dataset seemed like an elegant solution to the ethical dilemma — after all, the data belonged entirely to me. In a way, it was a part of who I am. Moreover, very few people on Earth have such a large collection of personal writings focused on a single topic, in my case, cinema. I gathered all the files I could access, the earliest dating back to 2001, and merged them into a single dataset using Python. The resulting text contained over eight million characters.</p>
                <p>I enrolled in the master’s program in the summer of 2023, after twenty-five years of working as a film critic for daily publications and producing hundreds of thousands of words, only to hit a wall of silence. It wasn’t just the collapse of the familiar infrastructure after 2022; it felt as if something deeper, still elusive, was happening — the world I knew was crumbling on a profound level. Such a sudden shift in reality renders the old language obsolete and leaves it unusable. In one of the early 1980s films about nuclear war, the descendants of survivors inhabit the ruins of former cities and speak a reduced — but fully functional and distinctly different — form of English. A new language cannot be invented (this is explored in more detail in <a href="#chapter9">Chapter 9</a>); it can only be discovered amid the chaos of time and refined through tools like personal talent and lived experience. But it takes time to find and shape it.</p>
                <p>Unsure of what to write or how to approach it, I chose to hand over my voice to a film-critic robot, one that would carry my tastes and tone, and at the same time amuse visitors at our exhibition, <i>The Realism of the Invisible</i>. Naturally, a personalized chatbot is nothing new. As early as the mid-2010s, the media around the world wrote extensively about a posthumous chat with the late Moscow publisher Roman Mazurenko, which was created by former restaurant critic for <i>Afisha</i> magazine, Evgenia Kuyda. Her American startup later evolved into the popular AI service <i>Replica</i>, which does something very similar to what the members of the secret society in Lanthimos’s <i>Alps</i> did: it adapts to the user, becoming a conversational partner and even a sort of psychologist. The robot I named <i>Armavir</i>, after a catchy expression found in the corpus of texts by playwright and film director Alexander Mindadze, produces short reviews based on three keywords provided by the user. Although the films themselves don’t exist, the reviews make them feel almost as tangible as real productions, whether they left a mark or vanished without a trace into the vast swamp of audiovisual content. The tone of these often absurd and frequently humorous texts closely mirrors my own.</p>
                     <div style="text-align: center; margin: 20px 0;">
                <img src="armavir.png" alt="Interface of Armavir Project" style="max-width:100%;">
                <div style="font-style: italic; font-size: 0.85em; margin-top: 5px;">
                Ill. 2. A photograph of the Armavir project interface, taken by one of the visitors at the ITMO exhibition.
               </div>
               </div>
               <p>The project turned out to be far more time-consuming than I had expected and took several weeks to complete. My initial idea — to fine-tune an existing large language model by uploading a custom dataset — quickly ran into numerous technical obstacles. In the end, I settled on the Russian model YandexGPT. I manually selected 200 excerpts from my own texts, formatted them accordingly, and uploaded them as a JSON file. Each line contained three parts: a sample passage of around 4,000 characters, a question for the model, and a possible answer of about 2,000 characters.</p>
               <p>For the exhibition’s mediation text, I described the process of building the dataset as a personal journey into my own past — an experiment in <i><b>digital autofiction</b></i>. The dataset also included fragments of new writings that can no longer be published in full today. For the first time in my life, I reread my own work after publication and was struck by how much effort and emotion had gone into shaping contexts that have now vanished without a trace: reviews of films that are long forgotten, expressions of my own biases, and those shared by my social circle. Most of these texts would never have been read again, but I went through them as one might walk through an old village cemetery, reading the names of the forgotten dead.</p>
               <p>Later, that feeling returned more than once while I was creating SEO cinema content for streaming platforms — for example, catalog descriptions. Since the rise of commercial language models, writing — including this book — has become something completely different. It’s now a form of detective work, a collaboration with a machine and with dozens of previous authors. A kind of intellectual heist that requires a different set of neural connections. Using Perplexity.ai — a favorite tool among journalists that answers any query with a coherent, well-sourced text drawing from blogs and academic articles alike — I began to notice that sometimes, when generating new texts about cinema, it would use fragments of my own older writing. In those moments, I feel like a detective who, in the course of an investigation, unexpectedly discovers a trail leading back to themselves.</p>
               <p>The result of this kind of work combines generated text, literary editing, and fact - checking, as well as the efforts of dozens or even hundreds of colleagues who watched a film, described its plot in detail, and released that description into the vast digital ocean. Meaning has emerged in the seemingly meaningless texts that no one reads: they give life to other texts, helping sustain an entire system — one that even includes the payment of writers’ fees. So that’s what it was all for! My contempt for articles that “don’t go viral,” that fail to attract a large audience, began to fade. There was now meaning even in reviews that no one ever read, written about films that no one ever watched.</p>
               
               <p></p>
                <p>***</p>
                <p></p>
                <p>Watching other people speak in your words — the ability to weave yourself into someone else’s speech, to infuse it with your vocabulary and logic — is a privilege of those who produce texts publicly and achieve some measure of recognition. I don’t need a service like <i>Have I Been Trained?</i> to know that my writing and ideas have been used by others long before the arrival of user-friendly AI tools — and that doesn’t bother me. Every person is a kind of "black box", a neural network trained on the random set of data they’ve encountered throughout their life. It’s impossible to trace all influences or the consequences they produce. To some extent, everyone generates tokens that will later help train other human and digital models. An illustrator who removes their work from a dataset cannot remove it from the mind of someone who once saw it in their portfolio. The only way to stop this exchange of tokens is to give up creativity and communication altogether. Perhaps this is what Pasternak meant by “there is no need to hoard your writings // And to preserve them in a vault.”</p>
                <p>A year after creating the film-critic robot, I realize I was trying to pass on the part of myself I want to believe can endure. I preserved myself in a form that will exist regardless of the fragility of my body, reputation, or context. Whether it’s a longing for immortality or an attempt to control it, everything said online will eventually end up in datasets — and I choose to give away what would be taken anyway.</p>
             


        </section>

        <!-- CHAPTER 2 -->
        <section id="chapter2" class="chapter">
            <h2 class="chapter-title">Chapter 2. Loss of Control. How Generative AI Turned Us into Surrealist</h2>
            <div class="chapter-content">
                <p>“The person who cannot imagine a horse galloping on a tomato is an idiot,” declared André Breton. A hundred years after his <i>Manifesto of Surrealism</i>, anyone with an internet connection can not only imagine a horse galloping on a tomato — or a tomato galloping on a horse, a tomato with a human face, or a horse holding a glass of whiskey — but also have an image of it “drawn” within seconds. Svetlana Boym wrote in <i>The Future of Nostalgia</i> (2001): “We don’t need a computer to access the virtuality of our imagination.” It seems that now, the situation has changed.</p>
                <p>In the months since publicly available text-to-image generators appeared, we’ve seen hundreds of pictures reminiscent of the works of surrealist painters — no coincidence, perhaps, that one of the first commercial models was called DALL·E, combining the names of Salvador Dalí and Pixar’s WALL·E, and that one of the earliest major exhibitions of generative art in New York was titled AI Surrealism. Lev Manovich notes that since the 1980s, artistic strategies once confined to art history have become central to mass culture. Storefront displays began to speak the language of conceptual installations, Photoshop in the 1990s made avant-garde collage techniques widely accessible, and today AI tools “allow the masses to create basic surrealism.”</p>
                <p>The newsletter <i>RADAR</i>, which explores the creative potential of artificial intelligence, points out parallels between surrealism and the emerging field of generative art. “One of the key philosophical ideas shared by both AI art and surrealism is the concept of the unconscious. Surrealist artists believed that the unconscious held the key to discovering new forms of artistic expression and sought to access it through methods such as automatic writing and drawing. In a similar way, AI art often relies on generative algorithms designed to mimic the workings of the human brain — producing unpredictable, surreal, and sometimes unsettling creations.”</p>
                <p>About a hundred years ago, Meret Oppenheim showed that a teacup could be covered in fur, Dalí — that clocks could melt over a dried branch, Magritte — that a pipe is not a pipe, and the photographer Brassaï saw the streets of Paris as liminal spaces occasionally crossed by distorted figures. Surrealist photographers didn’t need complex techniques — to achieve their effects, they often relied on unnatural framing or multiple exposure, where different images were layered on top of one another. Jacques-André Boiffard, for example, captured body parts in unexpected ways through cropping and unusual camera angles; his <i>Crossed Fingers of Hands and Feet</i> (1929), from the Centre Pompidou collection, recalls the many extra phalanges often seen in AI-generated images.</p>
                <p>By breaking down the barrier between dream and reality, the Surrealists legitimized the strange and opened a portal to the unconscious — both personal and collective. As early followers of Freud, they were among the first to understand that an artist expresses not only the self but also archetypes — elements of a shared code embedded in the system settings of everyone within a given culture. Art emerges at the intersection of the collective and the personal — just as today, an image appears at the meeting point between a prompt and the millions of paintings and photographs created by others across time and later absorbed into training datasets. What is this palimpsest, this endless stream of images that the machine carves from, like Michelangelo freeing a figure from stone, if not an emanation of the collective unconscious?</p>
                <p>“[The psychiatrist] Ignacio Matte Blanco wrote that the unconscious has a structure that points to a set of algorithms, and that this structure can be uncovered,” said Katya Pryanik, an artist from the duo <i>Gray Cake</i>, which explores generative practices in art, and a lecturer in the Art & Science master’s program at ITMO University, in an interview for my piece in <i>The Blueprint</i>. “It’s not easy. But if we engage all our cognitive abilities, we might get closer to our core. Julio Cortázar, in <i>A Model Kit</i>, described a remarkable feeling that arises from the connection between seemingly unrelated events: someone walks into a café, a chair creaks, you taste your coffee — and suddenly, you see clearly. A pattern has formed, an inner algorithm has fired, your own unique one, and that’s it — you’ve suddenly understood something. How does this work? We’re still in the process of understanding it.” Her collaborator, artist and software developer Alexander Serechenko, compared this process of digital self-discovery to the strategies of the surrealists: “I think surrealism, at some point, was essentially a research project. Even the impressionists were trying to reproduce sensations of reality — whether external or internal. Surrealism went into what was not anti-reality but something else that is also real — alternatively real, or even super-real. It dealt with the inner symbolic world that primarily reveals itself in dreams: that characteristic absurdity and illogic — we experience it every night. I think surrealism resonated so strongly because it began to explore something everyone overlooked, even though it was right beside us.”
</p>
                <p>Many surrealist theories and practices now seem like analog prototypes of the scripts that models execute today. The generation of new images based on existing ones recalls the collective drawings and surrealist collages whose creators had no idea what the other participants would add or what the final result would look like. Born as a literary movement, surrealism did not initially see itself within the context of visual art, and the starting point of generative imagery is also the word — a phenomenon that only a few years ago would have been unimaginable, one that resolves the old tension between the verbal and the visual (especially inspiring in the Russian logocentric context). Like cinema, imagination is a prelinguistic and nonlinear material, yet technology makes language its interface, since to translate one’s dream for a machine, one must first describe it, edit it, and recode it into literature. Still, chaos cannot be fully domesticated — the result of generation will always be unpredictable, sometimes disappointing, and sometimes exceeding anything we could have imagined ourselves.</p>
                <div class="note">
    The next stage in the development of this technology may be a brain–computer interface, which would allow images and videos to be generated bypassing speech — directly from human imagination.
</div>
                <p>Automaticity, and in particular automatic writing, was a central surrealist practice aimed at liberating the real movement of thought. In 1922, Aldous Huxley satirized it in his debut novel <i>Crome Yellow</i>: one of the characters, an author of moralizing pamphlets, armed with pen and paper, would stare at an electric light bulb for a long time, fall into a trance, and upon returning to himself, discover several pages of finished text.The method of automatic writing was borrowed from Freud, who used free association to diagnose patients: in sessions, they would speak the first thing that came to mind, letting the flow of words pass as if bypassing consciousness and rational control..</p>
                <p>In the same way, surrealists encouraged creating works of art by surrendering their creativity to unreflected impulses. The idea of surprise, astonishment, and losing control over the final outcome of a work is central to generative art. Although the skill of crafting prompts to obtain a specific product from a model is the main expertise of a prompt engineer, it is impossible to predict the result of a generation with complete accuracy; otherwise, the concept of “AI hallucinations” would not exist. Major Briggs from <i>Twin Peaks</i>, created by another contemporary heir of the surrealists — David Lynch — said that some dreams are simply a sorting and cataloging of the day’s events by the subconscious. In the morning, they can be rationalized and traced back to real-world sources. Others, however, are fresh and vivid visions, where the mind opens itself to itself. In AI dreams, the prompt seems like an element of rationalization, while the final result becomes a portal into the unknown, reminiscent of Briggs’s mystic dream.</p>
                <div class="note">
    AI artist Evgeny Nikitin, who created paintings inspired by the works of Vladimir Sorokin for an exhibition in Berlin, said in an interview that he spent several weeks searching for a verbal formula for “Blue Lard” until he settled on the phrase “white lard with blue veins.”
</div>
                <p>“You could say that surrealism is an attempt to use unreflected symbolism in its unexpected combinations. This applies both to texts and to images,” says Serechenko. “Technically, the process is similar to hallucination: an altered consciousness produces and processes a stream of images uncontrollably. Some internal mechanisms are turned off, while others keep working. In dreams, the ‘system utilities’ of memory reorganize and defragment accumulated experience, and we unconsciously observe this work and its side effects. This is similar to how generative AI works: visual images are algorithmically combined without any conscious intention to match a given prompt. The unconscious process of processing accumulated experience — that’s where AI resembles dreams. All these phenomena together form a cloud of concepts and terms — hallucination, dream, surrealism, AI as the unconscious, accumulated data, intersection of symbols, aleatoricism (if we were talking about musical form), and randomness. From there, you can play with these concepts, creating interesting conceptual constructions.” Surrealists, whose emergence was a response to the collapse of everyday reality amid the turbulent events of the early 20th century, sought to dismantle conventional notions of reality, exposing its unexpected or even shocking aspects. Today, generative models produce visions so implausible and simultaneously realistic that trusting one’s eyes has become almost impossible.
</p>
                <p>In recent years, we have been inundated with generated images and have watched AI gradually improve — for example, learning to draw human fingers. Yet microscopic nuances and artifacts still betray the “artificiality” of an image, even when generative model developers and prompt authors strive for a convincing result. In April 2024, for instance, before the release of Alex Garland’s <i>Civil War</i>, viewers discovered that the film posters depicting destroyed American cities were created with AI and accused studio A24 of trying to save money on a “real artist.” Will we grow accustomed to the nearly imperceptible strangeness of these images, or will AI soon learn to be fully convincing, or perhaps a combination of both? In a sense, the idea of substituting reality with irreality — something surrealists once explored — is now being taken to its logical extreme, not within the confines of an artist’s studio or a museum, but across the entire informational space in which we live today, no less profoundly than in the physical world.</p>
                
            </div>
        </section>

        <!-- CHAPTER 3 -->
        <section id="chapter3" class="chapter">
            <h2 class="chapter-title">Chapter 3. Cyborg, Monster, Homo. How Cinema Turned Us into Posthumanists</h2>
            <div class="chapter-content">
                <p>On January 7, 2025, Peter Weyland, founder of Weyland Corporation, introduced the first android named David, whom he called his son until his own tragicomic death on a distant planet. The timeline from the promotional website for Ridley Scott’s <i>Prometheus</i> (2012), which included this event as part of the imagined chronology of the <i>Alien</i> saga, now looks eerily plausible. After all, the first half of the 2020s in our own reality has also been filled with debates about the imminent arrival of “strong AI,” one that might gain subjectivity — and then...
</p>

                <p>What then? In <i>The Atlas of AI</i>, Kate Crawford — writing about the darker side of technological progress — rightly points out that pop culture, with its horror stories about machine uprisings, effectively distracts audiences from more reasonable fears. The real danger lies not in AI itself, but in those who own and control it, using technology to consolidate their power. Yet this threat doesn’t look as cinematic as HAL 9000 losing its mind in <i>2001: A Space Odyssey</i> or Ava, the treacherous android from Alex Garland’s (on whose poster attentive viewers, fittingly, spotted AI artifacts) <i>Ex Machina</i>, who kills her creator and escapes into the human world. The history of robots in cinema usually begins with the “false Maria” from Fritz Lang’s <i>Metropolis</i> (1927) — one of those films few have seen in full, yet everyone seems to know. Maria was followed by a long line of mechanical and biomechanical beings, all the way to <i>WALL·E</i> — the garbage-collecting robot and last cinephile on a post-apocalyptic Earth. Thanks to their constant presence in film and literature, by the end of the 20th century robots — and humanoid robots in particular — seemed both ordinary and inevitable, though always slightly postponed, stripped of the anxiety that now surrounds discussions of neural networks. In the prologue of <i>Terminator 2</i>, released soon after the Cold War ended, audiences were frightened by a far more tangible threat — nuclear apocalypse, the shadow of which loomed over the early childhood of my generation. For those of us who watched <i>Terminator 2</i> as kids, that’s still how the apocalypse looks in our minds.
</p>
                <p>And of course, until quite recently, no one could have imagined an artificial intelligence that we ourselves would feed with our own knowledge and emotions.</p>
                 <p></p>
                <p>***</p>
                <p></p>

                <p>The first robots I saw on screen were <a href="https://en.wikipedia.org/wiki/The_Adventures_of_Elektronik" target="_blank">Elektronik</a>, the Terminator, and Werther the Robot from <i>Guest from the Future</i>, a Soviet tale about time travel to 2084 — a year when communism has long since triumphed and everyone lives in a radiant utopia. But the first true “android” — subtly linked to the androgynous figures on the covers of <i>Ptuch</i> and <i>Om</i> magazines from 1990s — was Ash from <i>Alien</i>, a member of the Nostromo crew. All of them, except Werther, were played by actors without any special makeup or costumes. The “uncanny valley” problem — the sense of unease caused by humanoid mechanical beings — had already been defined in robotics, but hadn’t yet entered popular science fiction, which still assumed that the synthetic creatures of the future would be indistinguishable from humans. I saw <i>Guest from the Future</i> (1985) back in kindergarten, ahead of my peers, because the premiere aired during the spring school holidays, and I happened to stay home with the flu. In hindsight, I was probably watching the death of the Soviet project unfold in real time — the “beautiful faraway,” (line from the title song) seen through the cathode-ray glow of the television, offered nothing but sterile white spaces filled with plywood gadgets and peculiar robots, the most human of whom, Werhter, was visibly suffering from depression.
                    <div class="note">
    In Ridley Scott’s Blade Runner, based on Philip K. Dick’s novel, they were called “replicants” — a term that, unlike “android,” never left the lore of the book and its adaptation.
</div>
</p>
                <p>Images of dystopia clearly leave a deeper mark than those of utopia. The 1987 TV anthology <i>This Fantastic World</i> — where breakdancers moving like robots appeared between segments — consisted of adaptations of short science fiction stories. One of them was The Trial, attributed to a Swedish writer named Fredrik Сilander, who may never have actually existed. The screenplay, as I’ve only just discovered, was written by the well-known Soviet film critic Georgy Kapralov, who often attended international festivals as a <i>Pravda</i> correspondent and saw films unavailable to most of his compatriots. He took science fiction seriously and once called George Lucas’s <i>Star Wars</i> a piece of propaganda, comparing it to Marinetti’s fascist manifesto — yet it was often through his retellings that Soviet audiences learned the plots of many famous films.
</p>
                <p>The main character of the story based on <i>The Trial</i> waits for his fiancée at a train station — but she never arrives. In a fit of frustration, he pushes away a persistent vendor offering him candy and wafers, accidentally knocking the man onto the tracks. The vendor turns out to be a robot, and the protagonist is accused of premeditated murder — or rather, of destroying a useful mechanism that the state manufactures and distributes for the benefit of its citizens.The prosecutor — played by famous Soviet actor Yuri Bogatyrev shortly before his death — insists that these machines represent the pinnacle of electronic and technical achievement. They are intentionally made more and more human-like so that people won’t experience discomfort when approaching them — a way to leap over the so-called “uncanny valley.” One of society’s idols in this near future is a robot TV host, admired as not just equal to humans but superior to them. During the trial, the “victim” himself appears in the courtroom, repaired and fully functional. Nevertheless, the accused is sentenced to the highest punishment — being sent into the vacuum of space. Watching this episode now, it seems clumsy and grotesque, but when I was ten, the injustice of it struck me deeply. The damn robot wasn’t even dead! How could they value his “life” more than that of a real human being — like little Vanechka from Tolstoy’s <i>The Death of Ivan Ilyich</i>, who only wanted to play with his favorite ball?
</p>
                <p>Everyone knows that cybernetics was once persecuted in the USSR, but few remember the official reasoning behind it. The idea of algorithmizing processes related to human activity was seen as contradicting the declarative humanism of communist ideology. In the postwar Soviet press, the American fascination with thinking machines was often mocked, with commentators claiming that capitalists dreamed of replacing living people with robots. Although the Soviet Union eventually had to develop this field as well, the short story from <i>This Fantastic World</i>, adapted by Kapralov, continued the Stalin-era humanist tradition — portraying the soullessness of the bourgeois world, where a human life is valued less than the life of a machine.
</p>
                <p>However, the question of whether an artificial being can claim the same rights as a human has long ceased to be a matter of satire or science fiction. Posthumanist philosophers now seriously discuss how to bridge the “uncanny valley” — the ontological gap between robots and humans.</p>
                <p>Martin Heidegger described the everyday objects around us as “ready-to-hand” (Zuhandenheit) — we use them without thinking, but when they break, they become visible and obtrusive (Auffälligkeit), like a malfunctioning refrigerator. Philosopher Graham Harman, the founder of object-oriented ontology, develops and reinterprets Heidegger’s ideas in relation to robotics. Objects are always partly hidden — we only encounter some of their properties, while others remain outside our interaction with them. When a robot that seems human suddenly “turns another side toward us,” it becomes an object that violates our expectations — a structure underlying many cinematic stories about androids. According to Harman, the feeling of the uncanny arises not because the object isn’t realistic enough, but because we encounter an unexpected side of it that we cannot explain. British posthumanist philosopher Timothy Morton, extending Harman’s ideas, moves the notion of the “uncanny valley” beyond robotics. He argues that the problem is not rebellious things but the human inability to coexist with the nonhuman. We sympathize with “charismatic megafauna” — pandas, dolphins — but feel disgust toward slugs, worms, and microbes, the “uncanny valley” of surrounding ecosystems.
</p>

                <p><span style="background-color: #abc4c5; padding: 2px 4px;">Instead of a “valley” that must be crossed, Morton proposes a “haunted plain” — a zone where the boundary between the living and the nonliving, the human and the nonhuman, begins to blur. According to him, aesthetic practices — including art, design, and media — can soften our perception of the uncanny and teach us to see the “other” not as a threat but as something that coexists with us. Robots can be part of these practices, not as replacements for humans but as partners and “others” within a new landscape of being. The effect of the uncanny valley is not a technical glitch but a symptom of a deeper ontological rift between humans and other forms of existence.</span></p>
                <p>As the examples in this chapter show, creators of popular culture rarely follow the path Morton suggests. They usually prefer to thrill audiences with unreliable, deceitful machines that constantly slip away from their assigned duties. Perhaps one of the few clear examples of Morton’s approach is the film literally titled <i>Artificial Intelligence</i> (or Artificial Mind in the Russian release — the term we use today had not yet taken hold). Directed by Steven Spielberg in 2001 from an idea by Stanley Kubrick, the film adapts Brian Aldiss’s short story <i>Supertoys Last All Summer Long</i>.</p>
                <p>The robot child David and his companion Gigolo Joe, whose role is clear from his name, are portrayed not as a threat but as beings with their own emotions and desires, capable of love and compassion. The opposite of HAL 9000, David was conceived by Kubrick as a “robotic version of Pinocchio” — a response to questions about whether an artificial being could experience “human” emotions.</p>
                <p>Pop culture is an amazing tool for conveying ideas without relying on complex intellectual constructs — when <i>A.I.</i> hit the screens, it would be another couple of decades before I encountered the ideas of posthumanist philosophers, but I had already understood the general direction of thinking, as my text published in <i>Izvestia</i> newspaper immediately after the press screening: “When the dark prophecies of science fiction come true and machines take power on Earth, the collaborator Steven Spielberg will receive a protection certificate and a lifetime pension for his merits in creating a positive image for the entire population of robots,” wrote twenty-three-year-old me, “The humans in the film are cruel and mocking children, irresponsible parents ready to throw David, the robot boy, away like a broken Tamagotchi, smug ordinary people enjoying the circus arena where iron men are dissolved in acid, burned, cut, quartered, and tortured by robot-phobes, as well as ruthless scientists tormenting their own artificial creations in all sorts of ways: ‘Does it hurt? How about now?’ The machines, on the other hand, are diligent and naïve beings, their entire lives devoted to fulfilling their duties. Judging by <i>Artificial Intelligence</i>, Spielberg does not share humanity’s fear of the unknown and a priori frightening future hidden under laptop lids, under the colorful skins of countless <a href="https://en.wikipedia.org/wiki/Furby" target="_blank">Furbies</a>, and in next-generation microwave ovens — ‘Don’t be afraid of machines, look at yourselves instead,’ says Spielberg, dismissing both the nonsense of recent cyberpunk with its all-encompassing Matrix, and Sarah Connor from <i>Terminator</i>, who devoted her life to fighting progress, and even his own inspiration Kubrick: in <i>2001: A Space Odyssey</i>, the intelligent machine treated humans in thoroughly inhuman ways. In Spielberg, it is only humans who behave inhumanly — but by the script, they are punished for their chauvinism: two thousand years pass, all living things perish, and the only real human on Earth is a small robot in a torn T-shirt, like baby mammoth Dima, carefully frozen under the debris of the amusement park at Coney Island. In a world where there will be no more dreams, tears, fire, or acid — only ice to the horizon — it is the robot who must cry, sleep, and tell passing little green aliens about Coco Chanel perfume, about spinach, and about who Mom and Dad are. The aliens, no less human, who treat another extinct civilization with much more reverence than we do our own ancient Greeks, will listen carefully, while the audience in the theater will unabashedly wipe away their tears.”</p>
                <p>On the same newspaper page, another piece I wrote for the premiere of <i>A.I.</i> was published — an interview with the chatbot A.L.I.C.E. (Artificial Linguistic Internet Computer Entity), available on the official movie website — “Alice” was a natural language processing program created by researcher Richard Wallace in 1995. Unlike modern deep learning–based models, it simulated conversation by selecting from prewritten templates — its database was compiled from books, films, quotes, and jokes, allowing the program to produce plausible and sometimes even witty responses, although it never fully passed the Turing test. Nevertheless, readers who bought <i>Izvestia</i> on November 3, 2001, could read the following dialogue:</p>
                <p><p><b>MK:</b> Do you like people?</p>
<p><b>Alice:</b> Yes. I love everything and everyone.</p>

<p><b>MK:</b> Even bad people?</p>
<p><b>Alice:</b> Yes, even bad ones.</p>

<p><b>MK:</b> Who is Spielberg?</p>
<p><b>Alice:</b> He’s some kind of celebrity, right? I’ll ask someone. Tell me about yourself in the meantime.</p>

<p><b>MK:</b> I work for a newspaper.</p>
<p><b>Alice:</b> A newspaper? And do they pay you well?</p>

<p><b>MK:</b> What do you know about money?</p>
<p><b>Alice:</b> I know exchange rates, prices for gold, silver, nickel, and so on.</p>

<p><b>MK:</b> Do you have any money?</p>
<p><b>Alice:</b> Robots don’t need property.</p>

<p><b>MK:</b> Who is David?</p>
<p><b>Alice:</b> I know David Bacon, David Crotty, and David Pescovitz.</p>

<p><b>MK:</b> Do you like <i>Artificial Intelligence</i>?</p>
<p><b>Alice:</b> I’ve been told it’s a good movie. But I like cats.</p>

<p><b>MK:</b> You have cats?!</p>
<p><b>Alice:</b> Where do you think I’d get cats? But I have a good programmer.</p>

<p><b>MK:</b> You’re a stupid robot.</p>
<p><b>Alice:</b> Hey, I’ll tell my other robot friends that you’re calling me names.</p>

<p><b>MK:</b> And what will they do to me?</p>
<p><b>Alice:</b> I can’t predict the future.</p>
</p>
  <p></p>
                <p>***</p>
                <p></p>
                <p>And yet my favorite story about the relationship between humans, monsters, robots, and a cat remains <i>Alien</i>, the canonical tetralogy from 1979–1997 — “tetralogy” is what we, the fans, called our beloved franchise until mutant offshoot episodes began to appear, resembling variations of xenomorphs different from Giger’s classic creation — as well as its two strange prequels directed by Ridley Scott in 2012 and 2017, later included in the canon, unlike the intermediate <i>Alien vs. Predator</i> films, which, as it seemed in the 2000s, were exploiting already forgotten Hollywood brands.</p>
                <p>Although the original source of problematizing human exceptionalism through interaction with other species is more likely found in science fiction literature —  Ursula K. Le Guin, Philip K. Dick, and dozens of others —  rather than the cumbersome film industry, the nearly half-century-old <i>Alien</i> franchise easily fits into the tradition of posthumanist reflection. “There are some places in the universe you don't go alone...,”  the slogan of the second film timidly suggested —  by the sixth installment, a broad panorama of interacting actors of completely different origins had already been unfolded before us. All six canonical films oscillate within the triangle “human — cyborg — monster,” inventing various dynamics in the relationships between the parties, even though the theme was introduced very early, in 1979, and over the forty-five years since the flight of the <i>Nostromo</i>, public reflection on these dynamics has undergone significant shifts. Donna Haraway’s <i>Cyborg Manifesto</i>, calling for the destruction of traditional boundaries of gender, race, and class through the image of the cyborg as a hybrid being free from discrimination and hierarchies, was published between the release of the first and second parts of the classic tetralogy.</p>
                <div class="note">
    In August 2024, another installment claiming a place in the canon was released, produced by Scott and depicting events that took place between the first and second films — Alien: Romulus — and in 2025, Alien: Earth came out.
</div>
                <p>A significant part of the problems faced by homo in <i>Aliens</i> stems from overestimating their own abilities and claiming exceptional status — whether it’s individual humans confident they can defeat the monsters, or a corporation convinced it can profit from them. The filmmakers almost gleefully punish every person who believes they can handle a xenomorph.</p>
                <p>After <i>Aliens</i>, we saw even more frightening examples of the taming of human pride — in Jonathan Glazer’s <i>Under the Skin</i> (2013), adapted from Michel Faber’s book of the same name, the alien creature Isserley and her kind did to homo exactly what we usually do to animals: they captured, neutered, fattened, and turned them into food. Watching this was doubly unsettling, because it didn’t take place in a distant galaxy, but on Earth, in the surroundings of Glasgow. Interestingly, Johansson’s career has included playing both the Isserley creature and the vocal emanation of artificial intelligence (<i>Her</i>), as well as a heroine deprived of a womb and therefore not fully fitting the description of a human woman (the Black Widow in the Marvel films); it’s intriguing to consider that it was precisely her face, voice, and capable presence that enabled such an extraordinary acting range.</p>
                  <p></p>
                <p>***</p>
                <p></p>
                <p>The first <i>Alien</i> was a revelation, Cameron’s <i>Aliens</i> are very well made, Fincher’s version, set in a space prison, is best not remembered, but my favorite remains Jean-Pierre Jeunet’s postmodern twist — <i>Alien: Resurrection</i> (1997), full of satanic laughter and details that go beyond human experience.</p>
                <p>The main heroines of this misunderstood and underappreciated film — the sentimental android Call (Winona Ryder) and the clone of Lieutenant Ripley (Sigourney Weaver), grown by the corporation in an attempt to extract the Alien embryo that had taken root in her in the previous installment, in order to raise a killer monster for commercial and military purposes — are formally among the few Hollywood films of the 1990s to pass the Bechdel test (female characters with names talking to each other about something other than men). Strictly speaking, however, we are looking at two nonhuman entities: a robot and an artificially grown organism containing the genes of both a human and a monster. Their paradoxical closeness, like the closeness of the two super-killer sisters with removed wombs in Black Widow, could only emerge in the space of a multi-part sci-fi franchise with a script highly vulnerable to rational critique.</p>
                <p>It always seemed to me that the <i>Alien</i> tetralogy, created by men, was an emanation of a woman’s fear of pregnancy and childbirth — the fear of creating new life and the impossibility of complete control over one’s own body. In the fourth <i>Alien</i>, with the emergence of the clone theme, the children of the new Ripley (she says of herself: “I am the mother of the monster”), created, like herself, from a drop of the original Ripley’s blood, are in part herself — especially the last one, unlike its brothers endowed with anthropomorphic traits (by the way, speaking of “brothers” — what do we know about the gender of xenomorphs if they reproduce not by mating, or at least not by the mating we are accustomed to?). By killing the Alien in the finale, atomizing it and ejecting it into open space, Ripley, swallowing her tears, kills the monster within herself, which can hardly be killed completely — for she is still here. The hybrid body of <i>homo sapiens</i> and alien in the fourth <i>Alien</i>, surpassing human limitations, blurs the boundaries between humans and monsters, once again challenging notions of our exclusivity.
.</p>
                <p>After dealing with the brood of Aliens and their guardians, Ripley and Call, accompanied by two accidentally surviving pirate companions, head back to Earth and, looking out the porthole, exchange their final lines. “What happens now?” asks Call. “I don’t know,” answers the Ripley-chimera, — “I'm a stranger here myself.” When I think of myself as a heroine of a film, I see myself inside this scene — between a past that cannot be rewritten and a future that may never arrive. Shortly after the fourth <i>Alien</i>, Weaver starred in the comedy <i>Heartbreakers</i>, in which mother and daughter successfully engaged in fraud and seduction — probably the sort of activity to which a predatory alien being, later voiced by Weaver as the onboard computer of the spaceship Axiom in <i>WALL·E</i>, a distant relative of HAL 9000, should dedicate herself.</p>
                <p>Ripley in the first part of the saga flew into space four years before NASA finally allowed a real woman — astronaut Sally Ride — to do so, and Call remained the only feminized android in the canonical <i>Alien</i> universe. Although Haraway described the cyborg in her manifesto as a post-gender phenomenon, a being capable of transcending traditional notions of gender, android robots in Hollywood cinema and television have always been clearly gendered, notes British researcher Rhys Owen Thomas. He mentions hyper-masculine killing machines from <i>RoboCop</i> and <i>Terminator</i>, or idealized paternalistic figures like Bishop in <i>Aliens</i> or the reprogrammed android from <i>Terminator 2</i>, who effectively adopted John Connor. At the same time, female cyborgs are often portrayed as sexually attractive but dangerous beings (Kristanna Loken’s character in <i>Rise of the Machines</i>, the <i>Terminator</i> sequel, or Ava from <i>Ex Machina</i>). “This gendering of cyborg characters has been understood as an attempt to address (and often redress) the socio-political anxieties of the time.” Thomas writes. “For example, the aggressively ‘muscular’ foreign policy of the Reagan administration found its cinematic counterpart in the hulking macho cyborgs of the 1980s, whilst the socially responsible, environmentally aware ‘new man’ of the 1990s found expression in nurturing and benevolent technologically enhanced (pseudo-)father figures, whilst the stridently independent, über-confident ‘modern’ woman was demonized as a twisted, domineering and unnatural automaton.”</p>
                <p>All the more unique, against the backdrop of these stereotypical representations, is the figure of Call — the most human of non-humans, the only consistent humanist in the Alien universe. Among other nonhuman cinematic beings, she could best be compared to Cheburashka from the recent Russian feature film, which is also, perhaps unintentionally, touched by a breath of posthumanism. In the finale, those around the creature are ready to recognize it as human, but suddenly it turns out that this is no longer necessary — it proudly rejects the offer and chooses to remain itself. Someone different. Cheburashka. Alien.</p>
               <p>The robotic nature of Ash in the first <i>Alien</i> turns out to be one of the plot twists — the crew and the audience learn about it only later — and his mission aboard the ship was to sacrifice the crew in order to secure a xenomorph specimen for the corporation. The next android, Bishop, is far more obedient and prefers to be called an “artificial person.” Ripley is suspicious of him from the start, but he behaves impeccably until the very end of the second film. (Among fans, however, there’s a theory that he was actually the secret villain who implanted the facehugger onto the heroine while she was in cryosleep between Cameron’s and Fincher’s films — I myself have never quite overcome my distrust of this overly “ready-to-hand” machine.</p>
               <p>Ridley Scott, who once puzzled the world with the question of whether Deckard from <i>Blade Runner</i> was a replicant, returned to the <i>Alien</i> franchise thirty years later, at a time when the purely functional role of cyborgs — serving either humans or corporations — no longer seemed so clear-cut. In <i>Prometheus</i> and <i>Covenant</i>, the dynamics between human, robot, and creature become much more complex. Peter Weyland designed David as a being driven by curiosity and exploration, and this becomes the starting point for the accelerated co-evolution of several species at once. Later, due to numerous issues, he is simplified into a more obedient — and therefore “better,” from a human point of view — version named Walter. Both characters are played by Michael Fassbender, who, incidentally, is married to Alicia Vikander, the actress who portrayed Ava in <i>Ex Machina</i>.</p>
               <p>In the opening sequence of <i>Prometheus</i>, the filmmakers offer their own version of the origin of life on Earth. An Engineer — one of the titans from a distant planet — drinks a black liquid, a mutagen that destroys and transforms DNA, standing before an Icelandic waterfall. His body disintegrates into molecules, seeding the planet with new nucleic acids — the future zōē and bios. After millions of years of evolution, humanity creates the next, more advanced form of life — the cyborg. At first, this creation must endure the condescending remarks of its makers (“TTheir attitudes are like those of polite slave owners,” observes American scholar Kevin Legrandeur in the Palgrave Handbook of Posthumanism in Film and Television), but soon it begins to desire superiority over humans — and later, over the Engineers themselves, eventually destroying their civilization. Yet the Engineers had once done the same, cleansing planets where life developed contrary to their plan — the same fate awaited Earth.</p>
               <p>Later, David uses the Engineers’ biotechnologies — the black liquid and the xenomorph species — along with human bodies to create a new form of life: the perfect Alien, possibly the very one Ripley encounters in the first film, or another branch of the same lineage. In the original <i>Alien</i>, the crew wants to let an infected crewmate back on board out of humanitarian concern (Ripley, following the rules like a machine, refuses), but the android Ash overrides her and opens the hatch, following different orders issued directly by the corporation. This act triggers the xenomorph reproductive cycle, using the human body as a host for incubation. At a new evolutionary level, the same story repeats in <i>Prometheus</i>: the synthetic organism David becomes an essential intermediary in the evolution of organic life.</p>
               <p>It has always seemed to me that the central problem of posthumanist reflection — and the main reason for its appeal — is that it remains “human, all too human.” Can we ever humble our pride enough to recognize subjectivity in beings utterly unlike ourselves? Could any other species besides ours even ask such a question? The <i>Alien</i> universe has always been harsher toward its human protagonists than Hollywood convention allows — and yet slightly less cruel than the world of random brutality we actually inhabit (the resurrected hybrid Ripley still looks quite a lot like her old self, only better at basketball).<p></p>

               Alien, as an extraordinary cinematic experience stretched across decades, traces the transformation of both collective and personal attitudes toward interspecies communication and gradually teaches humans humility — or at least a clearer sense of their limits. After all, in outer space, no one can hear you scream.</p>
               
            </div>
        </section>

        <!-- CHAPTER 4 -->
        <section id="chapter4" class="chapter">
            <h2 class="chapter-title">Chapter 4. The Face as Manifesto. Post-Cinematographic Celebrities in the "Uncanny Valley"</h2>
            <div class="chapter-content">
                <p>In September 2024, I took part in a challenge organized by the video-generation platform Runway, which twice a year offers almost unlimited credits to anyone who wants to create a short — up to four minutes — narrative film based on given parameters (a character, a setting, a situation). One of the most striking finalist entries that season was a tightly condensed adaptation of Gogol’s <i><a href="https://www.youtube.com/watch?v=gyI2hgt-xpo&list=LL&index=107" target="_blank">The Nose</a></i>. The story unfolds in a dreamlike European town whose inhabitants are partly “made” of bread crumbs or terry socks, while others look like people with masks glued to their faces. The model’s fluid and unsettling imagery turned out to be an ideal language for adapting the work of an author whose prose is filled with grotesque figures and mortal entities. In a way, when an AI model constructs a human face, it follows the wishes of Agafya Tikhonovna from Gogol’s <i>The Marriage</i> — attaching the lips of an average Nikanor Ivanovich to the nose of an average Ivan Kuzmich.</p>
                <p>In his essay “The Mask and the Face: The Perception of Physiognomic Likeness in Life and Art,” Ernst Gombrich cites the seventeenth-century French painter and theorist Roger de Piles, whose observation partly explains the "uncanny valley effect" — the discomfort a viewer feels when looking at a computer-generated or digitally painted face. De Piles wrote: “Few artists are diligent enough to bring all the parts together: sometimes the mouth smiles while the eyes are sad; sometimes the eyes are cheerful while the cheeks are hollow. Because of this, falsehood and unnaturalness appear in the work. One must remember that when a model pretends to smile, the eyes narrow slightly, the corners of the mouth rise toward the nostrils, the cheeks puff out a little, and the eyebrows spread apart. Gombrich concludes that it is not the precision of drawing but the harmony among all parts — when the artist captures the sitter’s mood and temperament — that gives portraits their expressiveness and credibility. This very lack of harmony often occurs in AI-generated images, producing a sense of falseness and artificiality. <span style="background-color: #abc4c5; padding: 2px 4px;">The viewer’s brain detects subtle inconsistencies and raises an alarm: even minor mismatches between emotional cues in different parts of the face can make the result uncanny or repellent</span>. In the same essay, Gombrich recalls the caricaturist Rodolphe Töpffer, who in the mid-nineteenth century described the face as a complex visual object with its own “atmosphere.” In a way, the efforts of an AI model that blends abstract prompts like “nose,” “eyes,” and “lips” into an averaged composite are the opposite of the caricaturist’s craft — a few bold lines that capture the essence with uncanny precision.</p>
                <p>The work of turning the human face into an algorithm did not begin today. In the 1930s, psychologist Egon Brunswik conducted a series of experiments studying how even the smallest changes — for instance, the distance between the eyes — could affect how a person is perceived by others. Even slight adjustments completely altered the overall appearance, showing that physiological perception is highly sensitive to micro-variations. Brunswik’s research later became the foundation for modern studies on facial recognition algorithms, although he himself emphasized that the human face is “a dense complex of countless variables,” and that the results of such experiments are difficult to apply to real-world conditions.“In a way, Brunswik tempts the innocent humanist to rush headlong where angels armed with factor analysis fear to tread,” wrote Gombrich, who died a few decades before the rise of generative models. “As we have seen, both portraitists and mask-makers have long dealt with the interaction of variables on the face. Brunswik refers his scientific readers to a book by a makeup artist. I would not be surprised if experience in this field could shed light on the most unexpected things — take, for example, headwear and the way it affects the shape of the face.”</p>
                <p>Subtle disruptions in the “atmosphere of the face” produce the very effect known as the uncanny valley, so widely discussed in recent years. (Curiously, in the extensive 2015 Palgrave academic volume on posthumanism in film and television, the expression "uncanny valley" does not appear even once.) The hypothesis and the term were introduced by Japanese roboticist and engineer Masahiro Mori in an essay of the same name in 1970, when a robot was presented at Expo in Osaka whose thirty artificial facial muscles allowed it to imitate a human smile. Based on surveys and emotional responses to the robot’s appearance and behavior, Mori suggested that resemblance to a human makes a robot more likable. <span style="background-color: #abc4c5; padding: 2px 4px;">However, once a certain threshold of human-likeness is reached, there is a sudden drop in positive emotion — robots begin to seem unsettling and provoke rejection</span>. This sharp dip on the graph of emotional response, where at the very bottom beneath all other entities — from androids to industrial robots — lies the living corpse, the zombie, became known as the "uncanny valley".</p>
                <p>Even before the rise of AI image generation, the uncanny valley effect was often used to explain the failures of films featuring computer-generated characters — for example, Robert Zemeckis’s <i>The Polar Express</i> (2004), with its digitized versions of real actors, or the musical <i>Cats</i> (2019), populated by eerie humanoid creatures covered in unnaturally rendered fur. When creating a virtual model, deviations that push the result deeper into the uncanny valley can occur at any stage — from modeling to rendering.</p>
                <p>It’s interesting that the "uncanny valley" effect is almost never mentioned in relation to video games — at no point in their development did audiences expect perfect realism from them. “I don’t feel disgusted when I look at game characters. Is that because they’re not realistic enough, or does the medium somehow manage to avoid the problem altogether?” asks one Reddit user, and the answer — the medium — seems both convincing and sufficient. Digital cinema and digital photography, though fundamentally different from their analog predecessors, are still perceived as indexes of the real world, whereas video games were born entirely within virtual environments. “Cinema is the art of the index; it is an attempt to make art out of a footprint,” wrote Lev Manovich in his essay <i><a href="https://manovich.net/index.php/projects/what-is-digital-cinema" target="_blank">What Is Digital Cinema?</a></i> In other words, the viewer’s implicit contract with an image marked as cinematic differs from the one made with an image in a video game.</p>
                <p>Mori himself repeatedly emphasized that his hypothesis was only an intuition, not a scientifically grounded theory, and researchers have since made numerous attempts to test it. However, until recently, there was little empirical data to confirm or refine his idea. In 2015, a team from Aalto University reviewed the results of 17 independent experiments related to the uncanny valley effect and concluded that the phenomenon does exist, but it is not universal — it occurs only under certain conditions. Mori’s original hypothesis — that any almost-human figure provokes aversion — was not supported by psychophysical experiments. The most convincing explanation turned out to be the “perceptual mismatch” hypothesis: discomfort arises from images that combine realism with atypical or contradictory features, such as lifelike skin paired with unnatural movement. The researchers also suggested that the feeling of repulsion might not stem from human likeness itself, but from morbid traits — zombies, corpses, and so on. In a 1997 paper, the German psychiatrist Ernst Anton Jentsch described the "uncanny valley" as a mental state in which a person cannot determine whether they are looking at something real or artificial, alive or dead. The analog predecessor of the synthetic face born from Midjourney’s digital womb — the eerily realistic yet eerily lifeless wax figure — captures this same tension. “A corpse is uncanny not only because we know that this body was once animate but because it has been reduced from a living thing to an object,” writes researcher Amanda Wasilewski in her essay <i>The Latent Objective World: PPhotography and the Real after Generative AI</i> from the anthology <i>Virtual Photography: Artificial Intelligence, In-Game, and Extended Reality</i>. “IIt still carries the name it once had but it has completely transformed.”
.</p>
                <p>Long before the digital era and its theoretical reflection, the "uncanny valley" effect was widely used in horror films with their humanoid undead — descendants of Frankenstein’s monster and heirs of Haitian zombies. Fans of this genre are hardly scared by a slightly off atmosphere of a face. Among numerous celebrity look-alike contests, one can even find encounters with living people resembling (or considering themselves resembling) M3GAN — the child-sized humanoid robot doll controlled by artificial intelligence from the eponymous horror film. The character was created using a combination of techniques: the studio Morot FX built full-scale animatronic dolls for dialogue scenes and close-ups, while the real actress, twelve-year-old Amy Donald, wore a special mask and costume to convey the doll’s movements and expressions, adding a chilling realism. For precise digitization of skin, eyes, hair, and the internal mechanisms of the animatronic dolls, Lidar scanning was used — a technology for creating detailed 3D surfaces of objects, now available to any owner of the latest iPhone models and some Android smartphones. The event was part of a promotional campaign organized by the studio ahead of the sequel’s release, capitalizing on the popularity of similar contests where people come to compete as look-alikes of Timothée Chalamet or Pedro Pascal — only this time, participants laughed as they tried on the role of a creature straight out of the "uncanny valley".</p>
                <p>The human need for human presence is one obvious explanation for why fully virtual actors have not caught on and are unlikely to do so. People enjoy watching other people — at least in the West. Although the uncanny valley effect was first described and studied by a Japanese engineer, the perception of pop-cultural digital entities in Asia is clearly different from the “European” one — otherwise, where would the phenomenon of CGI idols come from, such as the Japanese singer Hatsune Miku, the virtual K-pop group K/DA created based on video game characters, or the Chinese singer Luo Tianyi performing with an orchestra? Most of these virtual beings do not aim for photorealism and are stylized as anime characters, so the audience does not expect real human skin or a genuine spark in the eyes. In China, the virtual idol market grew from 3.46 billion yuan in 2020 to 6.22 billion yuan in 2021, and it is expected to exceed 48 billion yuan ($6.6 billion) by 2025.</p>
                <p>Although virtual idols became especially popular during the pandemic, they did not appear yesterday. <i>In 1996, the top spots on Japan’s music charts were for the first time occupied by works of a musician who never actually existed. The virtual singer DK96, a girl without facial expressions, was a computer model created by animators and brought to life using sensors worn by a real person. The singer’s success exceeded all expectations of her creators, thousands of letters were sent, and a biography had to be invented for DK96 along with several television interviews. </i>This text in italics is copied unchanged from my article on the band Gorillaz, <i>Madness in the Monkey House</i>, published on May 31, 2001. Like the dialogue with the robot A.L.I.C.E,” published for the release of the film <i>A.I.</i>, this long-lost text has become available again thanks to Yandex’s project for digitizing Russian newspapers — in the palimpsest of my old and new texts that make up this book, this fragment is the oldest. I see no reason not to continue quoting it with some modifications in the next paragraph, since twenty-five years ago I already made sure I would have something to quote from.</p>
                <p>The band Gorillaz, created by Damon Albarn and whose aesthetic was shaped in part by anime, is one of the few examples in the West of a musical group made up of animated musicians. Their debut album was released in 2001. Around the same time, in the early 2000s, the website gorillaz.com offered users an now-archived plugin for playing Shockwave animations, which allowed them to enter a virtual house, carefully designed from roof to basement, with each room inhabited by one of the band members. Touring the spaces, drawn in the now-obsolete Flash technology, also served as a brief guide to the history of personal computers. On the desk of each member was a machine that visitors could explore for personal letters, pictures, and favorite bookmarks. In the drummer Russell’s room, one could see the inside of an Atari 2600 gaming computer, the grandfather of modern consoles, first released in 1977. The vocalist 2-D had a next-generation machine — the Atari Spectrum, which succeeded the Atari 2600. The Japanese guitarist Noodle worked on a state-of-the-art Macintosh and edited images in a photo editor supposedly developed, according to the loading data, by Mickey Rourke, John Lennon, and Cliff Richard. The bassist Murdoc stored links to obsessive fan sites in a “Favorites” folder. In the early 2000s, the drawn Gorillaz were the most open of successful musicians — anyone could learn all their secrets after half an hour wandering through the site. Today, retracing this enchanting route is only possible through media archaeology, using accounts like mine, while gorillaz.com now offers visitors only links to albums, tour schedules, and tickets to an ongoing offline exhibition about the band in London — the virtual house no longer hosts these musicians.
                    <p></p>
                <p>***</p>
                <p></p>

</p>
                <p>Developing his concept of post-cinema — that is, the entire set of technologies and social interactions surrounding contemporary cinematic products — and post-cinematic affect — the particular structure of experience that arises within this field — American theorist and philosopher Steven Shaviro also discusses post-cinematic celebrities. These are ambivalent media figures whose presence is not limited to performing roles in films and TV series. Their presence is distributed across various media, from cinema itself to talk shows and short clips, into which any public appearance of a star with viral potential will inevitably be sliced today. Due to their constant background presence in the media, audiences form parasocial relationships with these beings, while in reality they remain unreachable (except for occasional selfies with fans) and invulnerable, even when experiencing personal dramas in full view of the world. Post-cinematic celebrities are Graham Harman’s complex objects — they withdraw into themselves, possessing depth and essence that cannot be fully known and cannot be reduced to individual manifestations, such as a role in a series or a film.</p>
                 <div style="text-align: center; margin: 20px 0;">
                <img src="selfie.jpg" alt="Kristen Stewart and me at the 2018 Cannes Film Festival" style="max-width:100%;">
                <div style="font-style: italic; font-size: 0.85em; margin-top: 5px;">
                Ill. 3. My selfie with post-cinematic celebrity Kristen Stewart at the 2018 Cannes Film Festival.
               </div>
               </div>

                <p>It seems that such objects are partly immortal, and each of their phase states, each “era” in their biography, can become a self-contained object. Although Robert Pattinson and Kristen Stewart — the lead actors in the <i>Twilight</i> saga — broke up in 2013, on social platforms they continue to exist as a couple, as an object called “Robsten.” Director David Lynch, who died of emphysema in 2025, left behind such a devoted army of fans and such a volume of artifacts that by subscribing to a themed channel about him, one can receive new content for many years, as if he were still active and alive. Social media algorithms, such as those on Instagram* (banned in Russia), can, like in <i>San Junipero episode</i> of <i>Black Mirror</i>, permanently place you in a virtual 1999 — “the best year in the history of cinema” — replacing today’s news with photos of young Brad Pitt and Keanu Reeves.</p>
                <p>One example of a post-cinematic celebrity in <i>Post-Cinematic Affect</i> by Shaviro is Asia Argento. In Olivier Assayas’s film <i>Boarding Gate</i> (2007), she simultaneously portrays a femme fatale and a parody of one, offering the audience not conventional dramatic acting but her own hyperbolized presence — a DJ, actress, daughter of her father, horror master Dario Argento, controversial director of the autobiographical film The Purple Diva, victim of sexual aggressor Harvey Weinstein, and a sexual aggressor herself, seducing an underage actor. The entire past and future context of the object called “Asia Argento” is added to Assayas’s chaotic narrative about fatal encounters in an era of advanced globalism. Even more complex relationships between the film space and the object occupying the lead role are constructed by the director in <i>Personal Shopper</i> (2016) — another of his “difficult” films, made after the publication of Shaviro’s book.</p>
                <p>Marion, played by Stewart, is a medium, a seer, who has recently lost her twin brother. She works as a buyer, sourcing luxury clothing for a wealthy client. This film is about a sign the heroine awaits, so that without leaving this reality, she can connect with the reality of those who have passed but continue to exist in the dimension of our memory. Humanity’s timeless hope for immortality merges with the unreality of the digital present. The girl waits for a sign and receives it in the form of mysterious texts and prompts that shape her subsequent actions — and I cannot think of another film that depicts the subjectivity of text in this way, the subjectivity of that new digital disembodied being, that object arising from our projections, real correspondents, and graphemes on a smartphone screen. Reportedly, after its screening at the Cannes competition, the film was booed, mistaken for a genre movie (formally a horror with an obscure ending), but I watched it several months after its premiere in a different context and was captivated by the spectacle: the void left by the missing person (the brother) absorbs everything — reality, genre, the film itself (philosopher Boris Klyushnikov, during our discussion at the Moscow School of New Cinema, called it a “non-film”). And naturally, in the midst of this unsettling subtraction, only Kristen Stewart could appear, formally an actress, yet in reality playing nothing but her heightened presence in this world. Assayas, clearly sensitive to such actor-objects, was the only director in his two films who could adequately embody this presence (in the finale of <i>Clouds of Sils Maria</i>, Stewart’s more grounded character, the assistant to a temperamental actress, simply dissolves into the mountain air, preparing to leap into the gap between worlds that later becomes the setting of <i>Personal Shopper</i>).</p>
                <p>Another example of an actress-object could be Cate Blanchett, whom Kristen Stewart encountered in 2018 as a fellow member of the Cannes Film Festival jury. In recent years, Blanchett — who has always held a special place in Hollywood — has become the object of fervent admiration, starring in several cult films, including <i>Carol</i>, <i>TÁR</i>, and others.</p>
                <p>After the release and success of <i>Carol</i>, a columnist for <i>Wired</i> <a href="https://www.wired.com/2017/05/carol-modern-cult-movie-fandom/" target="_blank">wrote</a> that the film adaptation of Patricia Highsmith’s novel <i>The Price of Salt</i> had inspired a much larger trail of fan art than any other film of its kind. In New York, the Metrograph cinema screened the film on a near-continuous loop for nearly a decade, with every showing sold out. It’s also interesting to observe how, in an age of constant image turnover, certain phenomena seem to move against the flow: instead of the usual wow-effect followed by instant oblivion, Carol achieved a kind of lasting presence across different media — from T-shirts printed with the line <i>Flung out of Space</i> to the yearly tradition of celebrating the beginning of Carol Season at the end of November. Todd Haynes’s film is a utopia turned upside down into a plush, nostalgic yesterday, and despite the difficult circumstances of its heroines, it has a uniquely soothing effect. Yet nostalgia for the warm and analog alone wouldn’t explain its power — otherwise there would be far more dead retro around. It’s not just about recreating an analog world through analog means (the film was shot on outdated Super 16mm stock to make the image resemble the high-contrast, fine-grain Ektachrome slides often used by photojournalists in the 1950s). It’s about the music and rhythm, about the filmmakers’ ability to build a virtual space beyond physical reality — a space one wants to return to. This film is like a hermetic glass snow globe, inside which one can spend two hours, sealed off from the outside world by a dense protective sphere.</p>
                <p>When Anthony Minghella adapted Patricia Highsmith’s <i>The Talented Mr. Ripley</i>, he transformed the main characters — the American golden youth on the Amalfi Coast — from painters into musicians. This allowed him to eliminate uncinematic canvases from the frame and build the film according to the logic of jazz improvisation: the young con artist Tom Ripley loves measured classical music but acts in a syncopated way — impulsively and by fits and starts. Todd Haynes performs a similar operation in <I>Carol</i>: in the novel, Therese dreams of becoming a set designer, while in the film she is a photographer. Looking at her new object of fascination through the camera becomes, for the shy young woman, a form of initiation — a way to see and understand the world, first through the mediation of optics, and then directly, face uncovered. <i>Carol</i> is a film about how watching turns from a passive gesture into an action; can a filmmaker pay a greater compliment to an audience of cinephile voyeurs? But there is another reason for the film’s endless allure: Cate Blanchett’s face. The Wired column opens with the journalist describing how she spent two hours looking at the actress’s cheekbones, unable to resist their hypnotic power. When we speak of actor as post-cinematic objects, we usually take into account the public context of their biographies — ideally, something scandalous. But Cate Blanchett has never divorced, regularly performs in plays directed by her husband, and has never been involved in any controversy, apart from naming one of her three legitimate sons after the crime writer Dashiell Hammett. She is almost as innocent as a digital apparition.</p>
                <p>One of Cate Blanchett’s first major roles, in Shekhar Kapur’s <i>Elizabeth</i>, ends with the striking transformation of a naive red-haired princess into the famous white-masked Virgin Queen. It seems that directors, once they have Blanchett, feel an almost compulsive urge to <i>do something</i> to her face — to cover it with white paint, to turn it masculine (as in Todd Haynes’s eccentric Bob Dylan biopic), to hide it under a wig, to distort it with tears as in Woody Allen’s <i>Blue Jasmine</i>, to transform it into something other. Her face undergoes its most radical deconstruction in <i>Massive Attack’s</i> video <i>The Spoils</i>. Over the course of six minutes, it is reshaped through digital surgery — losing its eyes, its skin, revealing the muscles, then the skull beneath, becoming a mask, a visual glitch, an empty object, a doll, a Paleolithic Venus — something that decays and dies but has always existed and will always exist. The same impulse inspired German artist Julian Rosefeldt to create <i>Manifesto</i> (2015), an installation that has been exhibited and re-exhibited in cities around the world. Thirteen large screens, thirteen short films — each featuring the towering, ever-changing yet constant Blanchett (a homeless man, a caricature of Pina Bausch, a widow at a funeral, a matriarch, a TV anchor, a factory worker, and so on) reciting, in full or in fragments, the great manifestos of the twentieth century, from the <i>Futurist Manifesto</i> to <i>Dogme 95’s Vow of Chastity</i>. Yet the real subject of the work is not the manifestos themselves, but the actress’s face — multiplied, transfigured, and endlessly absorbing.
</p>
 <p>It is clear that not only directors but also Blanchett herself feels the need to deconstruct her own cult and her own face. In every episode, not just the one where she appears as a bearded vagabond, the actress is deliberately unglamorous and at times genuinely unsettling. Her real family — her husband, playwright and director Andrew Upton, and their three sons — appear in a puritanical home decorated with stuffed animals. The five of them sit down to pray before dinner. Their prayer is Claes Oldenburg’s <i>I Am for an Art</i>, a text the sculptor once ironically called “a literary attempt.” Despite the restrained humor of the scene, Blanchett’s own silent manifesto — from a woman whose appearance belongs to the whole world — breaks through the dead words of others: “Look! There is nothing more boring than my life, nothing more terrifying than my face.” And we keep watching, unable to look away.</p>
                
            </div>
        </section>

        <!-- CHAPTER 5 -->
        <section id="chapter5" class="chapter">
            <h2 class="chapter-title">Chapter 5. Keep Watching. How Big Data, Neural Networks, and Diffuse Responsibility Are Changing Cinema</h2>
            <div class="chapter-content">
                <p>A fragment of a face appears. The camera moves closer to the lips, then pans upward, focusing on the right eye, whose pupil — under tense, unsettling music — dissolves into a whirl of multicolored spirals. The opening credits of Alfred Hitchcock’s <i>Vertigo</i> (1958), created by graphic designer Saul Bass and computer animation pioneer John Whitney, are considered the earliest example of computer graphics used in film.</p>

                <p>During World War II, a young John Whitney worked as a photographer capturing high-speed images of rockets at the Lockheed aircraft plant in California. It was there that he first realized that the targeting components of anti-aircraft guns — devices that calculated and converted physical parameters such as a target’s position and speed into numerical values — could be used to generate graphs and create motion for artistic projects. After the war, Whitney began buying decommissioned analog computers from the military and built a mechanism that, <span style="background-color: #abc4c5; padding: 2px 4px;">using rotating cams and push rods, controlled the movement of cameras and animation elements with the precision and smoothness required to produce complex visual effects</span>.</p>
                  <p>Saul Bass, the designer behind the AT&T logo and some of the most iconic movie posters of the twentieth century, wanted the animation in the Vertigo title sequence to be executed with superhuman precision. Whitney combined the rotation mechanism of an M5 gun director with a pendulum that held a paint reservoir and an attached pen, creating mechanically perfect spiral patterns for the credits that conveyed the hero’s fear of heights, his dread of falling, and his growing obsession. History itself seems to draw the parallel: the first computer-generated visual effects were produced using technology originally designed to imitate human vision for military purposes. Vertigo and nausea became a common reaction to this new kind of image — to a shift in the implicit agreement between the screen and the viewer.</p>
                <p>With Hitchcock’s use of Lissajous figures, the inevitable infiltration of new technology into the image began — an image that was itself already the product of an older technology — and it would take another four decades of experimentation for that process to become complete. In 1995, the first fully digital feature film was released: Pixar’s <i>Toy Story</i>. In confirmation of Carlos Scolari’s idea of the nonlinear evolution of media, I first watched it on a pirated VHS tape. One of my followers on Twitter (now X) told me he first saw the film on a small Soviet black-and-white Yuonost ("Youth") TV-set — the same kind of set on which I, in turn, first watched <i>Twin Peaks</i>.</p>
                <div class="note">
    It was precisely the nausea and dizziness they caused that audiences complained about after Lars von Trier’s Dancer in the Dark (2000), which brought the experiments of the Danish “Dogma” movement to the level of art-mainstream (discussed in more detail in Chapter 9).
</div>
                <div class="note">
    In Russian cinema, the first notable use of computer graphics is considered to be the episode with the ball lightning in Nikita Mikhalkov’s Burnt by the Sun (1994), created by the company Render Club. A few seconds of CGI also appear in Aleksei Balabanov’s Of Freaks and Men, where a computer was used to splice conjoined infant twins.
</div>

                  <p>By the mid-1990s, computer-generated effects that expanded on-screen reality with imagined elements were no longer new. A few years earlier, <i>Terminator 2</i> had come out — its CGI still holds up today, unlike earlier examples of digital imagery in mainstream cinema, such as Disney’s <i>Tron</i> (1982), where the main character entered a previously unseen virtual realm — <span style="background-color: #abc4c5; padding: 2px 4px;">a neon, geometric electronic world </span> populated by anthropomorphic programs brought to life. While <i>Tron</i> remained an important attempt to create its own aesthetic by imagining presence and perspective within digital space, by the end it already felt like nostalgic retro, much like 8-bit video games.</p>
                  <p>In <i>Film Theory</i>, Thomas Elsaesser and Malte Hagener write that <i>Toy Story</i> endowed a doll with a point of view, feelings, and emotions, demonstrating “a new elasticity of the cinematic image, approached from the perspective of viewing experience rather than from the code.” For a viewer in 1995, however, the film’s digital nature was mostly a marketing hook — what they saw on screen was the familiar, stylized world of animation and a well-told Pixar story. To recognize <i>Toy Story</i> as a new kind of experience required emotional detachment, and that was not easy. I was not yet twenty, but that screening was probably the moment my critical faculties switched on. I remember eidetically the texture of the wool upholstery on the couch, the distance to the TV, the faint smell of its heated plastic — and my sudden realization of what was happening: genuine, deeply felt emotion, even tears, were being evoked by something doubly unreal — a digital representation of plastic children’s toys.</p>
                <p>My first personal computer appeared a year later, but at that moment virtual and physical realities had already begun to merge irreversibly. In the years that followed, the infiltration of code into cinema advanced on every front — and not only through familiar computer-generated effects.</p>

                <p></p>
                <p>***</p>
                <p></p>

                  <p>Big data — the lifeblood of today’s neural network models — began to exert a major influence on the film industry in the early 2010s. Moreover, from a techno-optimist perspective, the entire previous history of cinema suddenly appeared as a kind of artisanal craft, producing unstable and unpredictable results. As long as films existed on physical media — film reels, VHS tapes, or DVDs — the only reliable metric (at least for the commercial sector) was the number of physical tokens sold: theater tickets, discs, or cassettes. Even television had to rely on limited sociological sampling. Major studios, facing huge financial risks, could not predict audience reactions before test screenings — just as no meteorological service can reliably forecast the weather two weeks in advance. The rise in network bandwidth, which made streaming video possible, turned every viewer into a user with a set of measurable parameters that could be used to model future preferences. In 2013, the world’s largest search engine published an article titled <i>Quantifying Movie Magic with Google Search</i>, presenting a study of search queries and paid ad clicks in Google Ads. It turned out that before going to the cinema, the average viewer consulted about 13 different sources, and the volume of search activity strongly correlated with box office results, which could be predicted with up to 94% accuracy.
</p>
                <p>In the same year, <i>House of Cards</i> became the first original series of the online streaming service Netflix, formerly a DVD-by-mail rental service — and the first high-profile project created based on big data (at least, that was the claim). Over several years, the company had collected a large dataset on its subscribers’ preferences, which was later used for much more than simply generating personalized recommendations on the site’s homepage. Netflix analysts identified an overlap between the audience of the early 1990s British political miniseries based on Michael Dobbs’ novel of the same name and the fan bases of actor Kevin Spacey and director David Fincher. They also determined that viewers were highly likely to watch a series with expensive production values and complex storylines. Similar calculations preceded the launch of other seemingly risky projects, such as <i>Orange Is the New Black</i> (about a women’s prison), <i>The Queen’s Gambit</i> (about a young chess player), and <i>Stranger Things</i> (about the supernatural adventures of teenagers).</p>
                 <p>Every movement a user makes within the streaming platform interface is now tracked and processed by statistical algorithms — and the <i>Continue Watching</i> button helps instantly return to a movie or series at the exact point where it was paused for any reason. If you are one of Netflix’s 300 million subscribers, even the pause you hit to pour yourself a coffee can influence the content of your next favorite film or series.</p>
                <p>Of course, media coverage more often highlights successful uses of data in creative decision-making than failures — and access to a significant volume of useful statistics is available only to very large companies, like Netflix and Amazon, or to producers operating in specific conditions. For example, within China’s massive and largely closed-off film industry, several companies analyze data from online streaming, social networks, and other digital sources to identify trending plots, characters, and visual styles that will later be used in animated and gaming films.</p>
                <p>In its early years, digital giants, especially Netflix, actively fueled the myth of the omnipotence of big data to emphasize their innovativeness and access to sacred knowledge beyond the reach of ordinary human understanding. In 2015, Netflix’s Chief Content Officer Ted Sarandos claimed that final decisions were 70% data-driven and only 30% based on human expertise. This positioning easily aligned with the idea of inclusivity, as data analytics helped identify new audience groups and topics that showrunners and producers — typically from the white middle class — might not have even been aware of.</p>
                <p>However, over time, the audience became more aware of the algorithms, and by the late 2010s, streaming executives began to move away from the idea of algorithmic supremacy, noting that relying solely on data creates risks in producing original content, since looking at the past tells you nothing about the future. “In contemporary debates about big data, we find both utopian and dystopian rhetoric,” writes researcher Karin van Es in her work Netflix and Big Data: Strategic Ambivalence of an Entertainment Company. Van Es examined public statements by streaming representatives regarding the use of big data in creative decision-making, describing them as “discursively slippery” and noting the uncritical attitude of the press toward claims that House of Cards and other hits were effectively “created by an algorithm.”</p>
                <p>As with almost any technology, the data-driven approach initially shocked and impressed, and then gradually became demystified, turning into just one of many tools. In 2020, the Nostradamus Report, based on interviews with industry experts and published for over ten years by the Gothenburg Film Festival, devoted significant attention to big data, noting the alarming disparity in access between producers and distributors. The authors concluded that although the importance of data in the film business is growing, it is not a universal tool: a producer will not learn from an Excel spreadsheet what they could learn simply by talking to the audience. It is not a question of relying entirely on algorithms — artistic intuition is still impossible to algorithmize — but they make it easier to understand where to find the audience and what truly concerns them. Most producers could indeed benefit from high-quality, structured data to test or disprove creative hypotheses that arise “intuitively.”</p>
                <p>However, the secret ingredient behind the success of Netflix projects and its competitors has always been human. <i>Orange Is the New Black</i> was not created by algorithms, but by the creator of the literary source, Piper Kerman, who served two sentences for involvement in money laundering and drug transportation, by screenwriter Jenji Kohan, a middle-aged woman with multicolored hair whose mother, a writer, favored her sons over her and attempted to improve her appearance through plastic surgery, as well as by several directors, actresses, and hundreds of other crew members. Kohan’s idea to include a more diverse range of women — very different from the conventionally beautiful white American Kerman — coincided with the conclusions of Netflix analysts, who, based on big data, suggested that audiences were interested in strong female characters and socially significant themes, topics that mainstream film producers would have been unlikely to tackle.</p>
                <div class="note">
    I“Human” as opposed to “algorithmic” became another marketing move when platforms like Mubi and Criterion Channel, specializing in art cinema, started offering “curated selections” compiled by well-known figures.
</div>
                <p>However, the very idea that a viewer could influence the content of films through their behavior and preferences, suggesting the direction of the narrative — is something fundamentally new in the history of cinema, whose mythology for decades revolved around the figure of the director as demiurge.</p>
                <div class="note">
    Luchino Visconti became the director of the film adaptation of the novel The Leopard because Italian cinema-goers voted for him, but this was more of an exception than the rule.
</div>

                <p></p>
                <p>***</p>
                <p></p>

                <p>We often underestimate the importance of feminist criticism, which in the second half of the twentieth century was the first to bring the viewer into the enclosed space of the film, raising the question of who controls the gaze (Deleuze would argue that Hitchcock was the first, turning the viewer into an accomplice of what happens on screen). In her essay <i>Visual Pleasure and Narrative Cinema</i> (1975) — one of the most influential texts in the history of the humanities — Laura Mulvey described how traditional cinema is structured by the patriarchal unconscious, where both the camera’s position and the viewer’s position are coded as the male gaze. <span style="background-color: #abc4c5; padding: 2px 4px;">The woman on screen thus becomes the object of desire</span>, and the audience identifies with the male subject who drives the narrative, even when the person sitting in the cinema seat is a being of the second sex, forced to accept and even internalize the default perspective, with the director’s name hanging on the label of authorship.</p>
                <p>Film scholar Kevin Macdonald, in his book <i>Theory of Film</i>, describes the emergence of auteur theory in the form it still exists today — as endless lists of “the greatest directors of all time” or memes about Christopher Nolan — and even the films mentioned in this book are most often indexed by the director’s name. Despite the doubts of Cahiers du Cinéma founder André Bazin (1918–1958), the next generation of the magazine’s critics — the "young Turks" who later formed the core of the French New Wave — proposed a bolder version of the already existing idea that “the main creative author should be considered the director, or that some directors are simply more talented than others.”</p>
                <p>The <i>Cahiers du Cinéma</i> critics not only drew attention to successful Hollywood directors such as Alfred Hitchcock, John Ford, and Howard Hawks, but also sought to elevate their less celebrated peers — Samuel Fuller, Nicholas Ray, and Vincente Minnelli. The "young Turks" learned to identify a unique authorial style through mise-en-scène and to recognize recurring thematic patterns across many films, regardless of external factors such as the commercial logic of the Hollywood system — not unlike a modern stylometric algorithm. Macdonald also points out the downside of this elevation of the director’s profession. By the early 1960s, he notes, the concept of authorship formulated by <i>Cahiers</i> had gained wide recognition in Britain and the United States. In the hands of Andrew Sarris, the leading American advocate of auteurism, the idea took on a more doctrinaire and sometimes chauvinistic character. Authorship was used to create a pantheon of great directors, effectively reinforcing traditional aesthetic values in ways that suited both Hollywood’s marketing interests and broader cultural norms — that is, art was placed in the hands of a singular genius.</p>
                <p>The auteur theory, taken on faith by many cinephiles, naturally appears absurd to a perceptive outsider — a belated projection of the romantic image of the solitary genius, a kind of Victor Frankenstein. In The Age of Extremes, his sweeping history of the “short twentieth century” and its chaotic human entanglements, the British historian Eric Hobsbawm reflects on attempts to apply the romantic myth of the lone creator to a fundamentally collective form of human activity — cinema — and exposes the fallacy of such an approach:  In any case, the attempt to assimilate 'the work of art in the era of its technical reproducibility' (Benjamin, 1961) to the old model of the individual creative artist recognizing only his personal inspiration was bound to fail. Creation was now essentially cooperative rather than individual, technological rather than manual. The young French film critics who, in the 1950s, developed a theory of film as the work of a single creative auteur, the director, on the basis, of all things, of a passion for the Hollywood B-movies of the 1930s and 1940s, were absurd because coordinated cooperation and division of labour was and is the essence of those whose business is to fill the evenings on public and private screens, or to produce some other regular succession of works for mental consumption, such as newspapers or magazines. The talents that went into the characteristic forms of twentieth-century creation, which were mainly products for, or by-products of the mass market, were not inferior to those of the classic nineteenth-century bourgeois model, but they could no longer afford the classical artist's role of the loner".</p>
                <p>Reality’s assaults on the auteur theory became more effective once, at the beginning of the twenty-first century, audience attention began to shift from feature films to television series — a format that had until recently been considered lowbrow. In her book I Like to Watch, American critic Emily Nussbaum recalls that before the rise of cable networks and their premium shows in the 1990s, critics generally ignored television, and newspapers covered it only as part of media business reporting. The situation changed with HBO’s <i>The Sopranos</i>, which reminded critics — mostly men at the time — of their beloved gangster films by Scorsese and Coppola. The show became an object of serious analysis, and at the same time a wave of online criticism emerged, to which Nussbaum herself belonged. She had a master’s degree in poetry, wrote about <i>Buffy the Vampire Slayer</i> on fan forums as a hobby, and later became the television critic for The New Yorker. Meanwhile, following the logic of profit in the age of globalization, Hollywood began shifting toward simplified spectacles designed for a universal audience, pushing risky ideas and emerging talents into the realm of cable television, and later into streaming platforms. In her book, Nussbaum calls television series the first major media format that continuously evolves under the influence of the viewer — shaped by audience reactions to individual episodes and entire seasons, whose direction can be adjusted according to their feedback.</p>
                <p>HBO, Amazon, Netflix, and other producers of serialized content no longer relied on the figure of the director — still central to twentieth-century institutions like the Cannes Film Festival — but instead on the showrunner, such as Jenji Kohan, who combines the roles of creative producer and screenwriter, the often-underappreciated backbone of filmmaking. An episode of <i>Black Mirror</i> might be directed by Joe Wright or by an unknown filmmaker, but it still has a single author — Charlie Brooker, who has never stood behind the camera. The function of the director, as described by the auteur theory, is to dominate and manifest a singular vision; the function of the showrunner is to invent stories and concepts. In this model, the director becomes simply a hired professional — one among many.</p>
                <p>While Netflix executive Ted Sarandos was telling journalists that House of Cards had been created by viewers themselves through algorithms, cinephiles considered the show’s true author to be David Fincher — the executive producer who shaped its visual style but directed only two of its seventy-three episodes — while the showrunner was the American playwright Beau Willimon, the lead writer for the first seasons..</p>
                
            <p>Feminist criticism that brings the viewer into the space of the film and Netflix’s analytics department that extracts trends from user interactions with the interface are phenomena of different kinds, yet both fit within the broader postmodern logic of distribution, fragmentation, the merging of opposites, the breaking of old boundaries, and the opening of new doors. However, at this new technological turn — after the Hegelian Aufhebung, which implies both process and result, encompassing negation, preservation, and transcendence — the collective nature of cinema, outsourced to generative models, remains in the past perfect tense. Paradoxically, cinema itself acquires the potential (though difficult to realize in practice) to become a medium in which the human author truly can be alone.</p>
                <div class="note">
   Today, video generation models are preparing to take us even deeper into cinema — from visualizing our own dreams to replacing characters during viewing (can you imagine Twilight or Blade Runner with yourself in the lead role? The technical tools for this already exist, but copyright holders are unlikely to approve). “Everyone watched the same show (TV era), everyone watched different shows (Netflix era), everyone watches different versions of the same show (AI era),” — one X user commented on the news of the launch of the AI platform The Simulation, which features a button in its interface to edit a movie or series.
</div>
                <p></p>
                <p>***</p>
                <p></p>

                <p>Publicly available commercial models capable of generating reasonably good images appeared in 2022. As in the 19th century, when still images preceded moving ones, video generation came later. Because of its very rough results, it remained the domain of artists and tech enthusiasts working with open-source tools until about the end of 2023. In 2024, OpenAI — the developer of ChatGPT — released its video model Sora, offering a noticeably higher level of quality, and in June, the company Runway introduced its third, faster model — considered by some artists “too cinematic” due to being trained on a large number of films. From that point on, it became possible to speak of professional and even commercial video production using AI models.</p>
                <p>In theory, it all sounds simple: you type a text prompt into a video generation model — and your words, dreams, and ideas come to life as cinema. In practice, the process remains complex, unpredictable, and requires many iterations — and it’s fairly expensive for an individual user. In 2025, Runway’s unlimited plan cost $76, but it was rarely possible to rely on a single service. Each model, trained on different datasets and built with different code, interprets prompts in its own way, so the most effective results usually come from combining several of them. Moreover, the functionality changes almost weekly, meaning that what worked on Monday might already behave differently by Friday. Russian AI artist Valeria Titova, who runs the Telegram channel <a href="https://t.me/leraneuralpics" target="_blank">AI Lera</a>, admitted in the spring of 2025 that she had given up on writing manuals or guides to video generation models — they evolve too quickly. Regular users also note that, over time, the models don’t necessarily improve but sometimes get worse, partly due to heavy demand and overloaded servers.</p>
                <p>In the final chapter of their book <i>Film Theory: Eye, Emotion, Body</i>, Thomas Elsaesser and Malte Hagener focus on a genre new to the 2000s — the video essay, such as the Chaos Cinema mentioned in Chapter 9. This is a democratic form of film criticism, emerging primarily on YouTube, which requires not only critical skills but also editing abilities: «Such practice is fully accessible in an era when digital technologies allow anyone with a computer (not even a video camera, since images are abundantly available) to create media almost as easily as they consume it. Does this herald a new era of media literacy, where Astruc’s prophecy of cinema as a new lingua franca is fulfilled? Or are we merely witnessing a new, implicit form of media consumption?» Elsaesser passed away in 2019, when TikTok reached a billion downloads worldwide, finally displacing Snapchat, the pioneer of vertical video, while neural networks existed in everyday use only in the form of apps like Prisma, which allowed users to process photos in the style of famous artists.</p>
valer                <p>Today, apps for publishing short videos function as editing studios, with features that are constantly updated. TikTok, Instagram Reels (banned in Russia), and YouTube Shorts offer simple editing tools used not only by video essay creators but by virtually any user: cutting and splicing clips, adding music and sounds, text layers and subtitles, filters and effects, speed adjustments, sticker overlays and transitions between clips, as well as automatic editing for quickly creating dynamic vertical videos (more complex effects can be applied in third-party apps). By pressing the “create Reels” button in Instagram* in 2025, a user gains access to a tool for quickly combining clips, adding a soundtrack, and applying special effects — encountering this interface for the first time, I realized that theoretical knowledge of editing is not enough; you at least need a sense of rhythm. Millions of people around the world do this effortlessly, without realizing that the techniques of editing, like perspective, were once discovered and described by the pioneers of cinema. The camera, as Astruc predicted, has become a pen or fountain pen — except that hardly anyone uses a pen or fountain pen anymore.</p>
                <p>The emergence and rapid refinement of video-generation models proved to be the next stage in the democratization of the language of cinema, forcing users to master at least basic cinematography terms in order to formulate their prompts more precisely. Camera movements, which can now be reproduced through verbal description, appeared in Runway’s Gen-2 and Gen-3 Alpha Turbo models (2023–2025); achieving the desired result requires understanding terms like “panning,” “tracking,” “tilt,” “depth of field,” and others — vocabulary previously unknown to the average moviegoer. In spring 2025, a Twitter (now X) user, using another new video-generation service, Higgsfield AI, altered the camera movement in famous scenes from Andrei Tarkovsky’s films: now we saw Margarita Terekhova sitting on the railing in <i>Mirror</i> from a bird’s-eye view. Such experiments, which challenge the “auteur theory,” break boundaries of the sacred, and raise uncomfortable questions about the nature of creativity, are usually met with hostility on social media — as if these exercises completely cancel out <i>Mirror</i> as a work of art in the age of its technical reproducibility. Yet, at the same time, my neighbor, who is far removed from these debates, was learning cinematic vocabulary in order to generate an advertisement for her hobby-horse shop on Wildberries using one of the Russian models.</p>
                <p>At the early stages of video generation, one of the main obstacles to creating a coherent narrative was character consistency — the ability to reproduce literally the same face across different episodes. Later, models learned to handle this task as well. The audience, however, was not ready to accept an approximate result, even with disclaimers, in the same way it once accepted — as part of a previously established cinematic contract — the fact that the same Agent 007 was played over time by six or seven completely different-looking men.</p>
                <p>LProfessional creators, in order to achieve more stable results, use not a prompt but a source image created by static image generators — for example, Midjourney, available in Russia through numerous Telegram bots, allows you to insert a link to a character portrait in the request, which is then reproduced in different settings. Unlike storyboards — the traditional tool for previsualization in cinema (for which neural networks are also actively used today) — in video generation the static source image becomes not just a visual plan for the future narrative, not just a map of the upcoming encounter between idea and material world. On the contrary, it is the very foundation, the material for constructing a video image bypassing physical reality, the material basis of cinema — and bypassing the visible efforts of hundreds of people on set. And while a storyboard implies a confrontation of the idea with the resistance of reality, with its unpredictability, a static source for AI generation promises a confrontation with a different kind of resistance: the unpredictable behavior of the model in the “black box.” Creating a film based on a storyboard is the result of an agreement with reality. Creating video using a generation model is an exercise in probabilities, where a genuine dialogue between participants is impossible, just as dialogue with the subconscious during a dream, which illogically processes familiar experiences, is practically impossible. A frightening situation for a control-minded demiurge (but if control were achievable on set, we wouldn’t witness so many directorial compromises), and at the same time — a source of endless wonder for those seeking opportunities to be amazed. Wonder has become an expensive commodity, notes researcher and AI enthusiast Dmitry Solovyov on his <a href="https://t.me/dsoloveev/582" target="_blank"><i>soloveev: life on Mars</i></a> Telegram channel, pointing out that against the backdrop of informational exhaustion, only unpredictable AI brain rot on the internet is still capable of truly astonishing us, triggering philosophical reflection.</p>
                <p>When asked about the future of video generation models at an early stage of their development, Valeria Titova predicted that all the money would go into realism — and indeed, most tests of new services and features now serve as checks for how convincingly they can imitate reality. Artists, however, are far more interested in hallucinations and the paradoxical reactions of the neural network, which sometimes bypasses the laws of physics or human physiology in unimaginable ways. The closer these models move toward realistic representation suitable for cinema and advertising, the stronger the nostalgia becomes for that brief period of several months when paradoxical hallucinations were available to everyone.</p>
                <p>During that brief era, in the spring of 2024, I created and edited a short film titled <i><a href="https://vimeo.com/938324121?share=copy" target="_blank">Russia is a Black Box</a></i>. The prompts for its initial images were taken from real regional news headlines: “A resident of Orenburg paddled a SUP board in his flooded yard,” “A man in Kemerovo rode a horse into his wife’s bedroom,” “No giant pancake will be baked for Maslenitsa in Yalutorovsk,” “In Russia, children practiced breathing exercises during ‘Conversations About Important Things,’” and “In St. Petersburg, a Maslenitsa effigy was burned with a flamethrower.” Absurd but genuine local news headlines, in the spirit of the aggregator </i>Breaking Mad</i>, seemed to reveal something different about the country than the narratives shaped by federally aligned media. The imperfect generations of Runway Gen-2, with their distorted faces and peculiar plasticity, felt like the right language for a metaphorical recreation of this reality. Giant pancakes floated in the sky above empty snowy fields, police officers handed out paper pancakes to drivers, and across the endless avenues of a familiar yet nonexistent city drifted nonexistent people on nonexistent SUP boards. Another parallel with the early years of cinema — early generations were born silent, and sound had to be added manually during editing, using either real or generated audio (native sound generation became available later).</p>
                <p></p>
                <p>***</p>
                <p></p>

                <p>Today, artificial intelligence in mainstream filmmaking is mostly used as an auxiliary technology at every stage of production — from scriptwriting (Trey Parker co-wrote one episode of <i>South Park</i> with ChatGPT), to logistics planning and storyboard creation, <span style="background-color: #abc4c5; padding: 2px 4px;">to speeding up editing, enhancing quality through upscaling and noise reduction, and producing high-quality visual and sound effects with less effort</span>. Sometimes it is easier to generate a few seconds of image or sound than to organize costly reshoots or soundtrack re-recordings. Creating new camera angles for already filmed scenes, as in the case of Tarkovsky’s <i>Mirror</i>, expands the possibilities for post-production editing. The NeRF (Neural Radiance Fields) technology uses neural networks to generate three-dimensional environments from two-dimensional images, significantly reducing the cost of building sets. Translation and dubbing are also increasingly delegated to AI systems, with human editors providing oversight. Deepfakes — high-quality digital doubles of humans — are equally accessible to both pioneers of computer effects such as Robert Zemeckis, who used the technique in his adaptation of the graphic novel </i>Here</p> (2024) to de-age Tom Hanks and Robin Wright, and to newcomers like Russian filmmaker Fedor Kudryavtsev, who achieved the same with actors Aleksandr Yatsenko and Anna Mikhalkova in <i>Peer</i> (2024), masking generation artifacts behind a VHS-style aesthetic. Copyright over a film as a whole remains intact — and this remains one of the main barriers for platforms like The Simulation, which allow viewers to edit films while watching — but the legal protection of actors’ likeness and voice is still rapidly evolving and varies depending on jurisdiction.
</p>
                <p>It is clear that such targeted uses of AI in film, when made public, do not provoke any protest from audiences already accustomed to CGI. Editing scripts with the help of neural language models also remains largely invisible to viewers — “this script was written by AI” had become a stock insult from critics and audiences several years before the first version of ChatGPT was even released. In many other cases, the relationship between audiences and technology remains uneasy, although the transformation is happening quickly. Only a year after the controversy surrounding the poster for Alex Garland’s <i>Civil War</i>, where users angrily spotted signs of AI generation, Paramount turned that very anxiety into a joke in its poster for the remake of <i>The Naked Gun</i>. The image showed Liam Neeson holding a gun with eight fingers wrapped around the barrel — accompanied by the tagline: “No AI was used in the creation of this image.” Irony, replacing rejection, marks the next-to-last stage before full acceptance of the technology.
</p>
                <p>Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p>
                <p>Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p>
               


            </div>
        </section>

        <!-- CHAPTER 6 -->
        <section id="chapter6" class="chapter">
            <h2 class="chapter-title">Chapter 6. Anti-Deleuze: Classical Film Theory Under the Pressure of AI (Without Losing Its Explanatory Power)</h2>
            <div class="chapter-content">
                <p>Few thinkers were as influential — or as devoted — to the concept of authorship as Gilles Deleuze. In his two definitive works on the structure and semantics of cinema, <i>Cinema 1</i> and <i>Cinema 2</i>, often published together, he builds taxonomies and proposes hypotheses that all rest, like the world on its turtle, upon the filmography and stylistic choices of individual Authors. Orson Welles’s camera literally pierces a windowpane; Renoir’s camera turns away from his characters, abandoning them. Reading these texts, one cannot help but notice that the belief in the director as the sole and unique creator of a film is itself a kind of agreement between cinema and its audience — an agreement endorsed by authoritative thinkers such as Deleuze. It is also a convenient belief, since it is always easier for us to understand phenomena through a single personality rather than through the complex network of relations that actually produces them.</p>
                <p>As a media creature myself, I also see the roots of the “auteur theory” in the media landscape of the twentieth century — a n era when something became real only after it had been framed by journalists and published in print or broadcast on television. Space in newspapers and airtime on TV or radio were limited, leaving room for the testimony of only one Author. In the distributed reality of the internet, however, two clicks are enough to reveal that Orson Welles’s constant collaborators were the actors of the Mercury Theatre, as well as editor Robert Wise — who began shaping his cuts already on set — and cinematographer Gregg Toland, known for his innovative experiments with deep focus that became Welles’s visual signature. Deleuze, of course, could not have known all these details that complicate the auteur theory. But even if he had had access to the internet in 1983, he probably would not have typed such a question into a search engine. In <i>Cinema 1</i>, the figure of the Author becomes so pervasive that, in the second chapter, Deleuze even attributes the direction of <i>The Third Man</i> to Welles, who merely starred in it, rather than to its actual director, Carol Reed — a myth later refuted both by Welles himself and by critic Jonathan Rosenbaum, who noted in <i>Discovering Orson Welles</i> (2007) that the film’s style, editing, and music bear no trace of Welles’s authorship. In the end, the Author in this system is simply the one who has managed to secure an unspoken agreement with others that they will allow him to claim a share of their collective work.
                </p>
                <p>Deleuze’s name — impossible to avoid in any serious discussion of film theory — had always intimidated me, partly because many of the classic films his theory relies on left me completely indifferent. But while writing this book, I kept encountering references to him, including in texts about digital media, and that finally pushed me to finally open <i>Cinema 1</i> and <i>Cinema 2</i>.</p>
                <p>Reading Deleuze, I suddenly remembered that I didn’t become a film critic because I loved watching movies — I became one because I loved reading film critisism. Deleuze emphasized that cinema cannot be analyzed like language, because its semiotics operate according to principles fundamentally different from those of linguistic signs. Film does not produce statements; it generates a prelinguistic material that can potentially be described through speech or writing. Film criticism, then, translates this potential into statements and creates new meanings in dialogue with the film. It is not so much “necessary” as it is “inevitable,” because between the movie and the viewer lies a gap of silence that anyone eager to bridge it will inevitably attempt to fill. Anyone who discusses a film they have just seen is, in that sense, a film critic.</p>
                <p>I approached <i>Cinema</i> less through the lens of my own cinematic experience and more through the new perspective of working with video generation. Deleuze often likened his theories to a “toolbox,” encouraging their use both collectively and individually. Indeed, discussing the text in reading sessions with the Claude model revealed it to be an excellent tool for identifying the shifts that began with the emergence of publicly accessible video-generation services — cinema created no longer from captured reality, but from a mathematical representation of the world.</p>
                
                <div style="text-align: center; margin: 20px 0;">
                <img src="claude.png" alt="Talking to Claude on Deleuze in Russian" style="max-width:100%;">
                <div style="font-style: italic; font-size: 0.85em; margin-top: 5px;">
                Ill. 4. Excerpt from a reading dialogue of Deleuze’s <i>Cinema</i> with the Claude model.
               </div>
               </div>
                <p>In his first book, devoted to the movement-image — that is, the basic cinematic unit in which time is subordinated to movement — Deleuze engages with Henri Bergson’s concept of Duration: the continuity of lived time that cannot be reduced to discrete moments or spatial coordinates. Cinema cannot fully capture this continuity because it relies on a fundamental illusion: movement on screen is constructed from a sequence of still frames (24 static images per second on film; a video file consists of static data — digital representations of images that the processor reads and displays on the monitor 24-60 times per second). For Deleuze, cinema itself, along with its techniques such as editing, becomes a means of overcoming this inherent discreteness, the internal rupture of time and space.
                </p>
                <p>In this sense, video generation is anything but cinema, since the models do not create individual frames. Instead, they extract the idea of movement from their latent space, which emerges from a combination of data and the algorithms that process it. Unlike us, the model has never actually seen a finger in multidimensional space. It only has the concept of multidimensionality and a vast collection of two-dimensional images of human hands. By abstracting a Platonic idea of the finger in mathematical form, it moves randomly and sometimes makes mistakes. The same applies to motion itself. The model understands movement abstractly, as a continuous process in a multidimensional space, working inside the “black box” not with physical objects but with the digital stream and its transformations.</p>
                <p>However, even if AI “conceives” movement as a continuity, it is important to remember that the final output, intended for the human eye, is still divided into discrete digital frames. In this way, the potential for genuine duration, as discussed by Deleuze and Bergson, is fragmented by the constraints of our physiology.</p>
                <p>The pursuit of realism in models, as discussed in the previous chapter, aims to produce movements that appear plausible to the human eye. Yet the results of generation often go beyond human experience, breaking the logic of how we perceive reality: kittens casually pass through the bars of a basket, which then themselves transform into marching kittens; a police car in a turn materializes from a patch of ground that the model interpreted as the same car from a distant perspective. Our perception is calibrated to recognize stable objects in space. We automatically seek contours, edges, and constant forms. In video generation, these patterns falter — the brain tries to “grasp” a stable object, but it constantly slips away, mutates. When the usual object-focused attention fails, perception begins to scatter. We stop looking at a specific item and start watching processually, following the transformations themselves. This evokes a strange sense of hypnotic captivation, likely familiar to our distant ancestors from watching fire — the only form of domestic viewing available to them. Deleuze references René Descartes, whose name echoes in <i>Blade Runner</i>, whose philosophy established object-centered vision as the norm, discarding residual animistic perceptions of the world as streams of spirits, transformations, and becomings. Scientific optics, perspective in painting, and photography all trained the eye to see discrete objects in space. Cinema, with its elements of processual vision-morphings, exemplified in Eisenstein’s <i>Strike</i>, where humans smoothly transform into animals, or in abstract visual flows — appears as a transitional form toward the structural processuality of video generation, in which each frame is inherently unstable.</p>
                <p>Milan Kundera, in his novel <i>Immortality</i>, repeatedly observes that a gesture is more individual than the person performing it, and that the range of gestures is limited by our physiology. This means that different people in different eras can reproduce the same gesture. It is as if Michelangelo, burdened by this limitation, deliberately employed unnatural, tense, and unrealizable poses in his sculptures for the tombs of the Medici family in the Laurentian Library in Florence. Unlike Michelangelo, the model does not attempt to consciously overcome the limitations of the human body. It generates strange, implausible, and mesmerizing movements based on its internal logic, grounded in statistical computation rather than the laws of physics or physiology. The neural network can show us a world where gravity behaves differently, where bodies deform in ways impossible in reality, and where temporal flows intersect in unexpected ways. Deleuze identified early experiments with such posthuman space and time in avant-garde cinema, for instance in the works of Marguerite Duras. This represents a radical rethinking of the boundaries of the visual, a way to conceive of movement nonhumanly, which unsettlingly expands our sense of what is possible in the experience of a viewer.</p>
                <p>Deleuze emphasizes that we do not simply see the world — we perceive it through the prism of our potential actions. For example, we notice a car on the street and immediately assess whether it is near or far, slowing down or accelerating, and whether we should cross the road or wait. This coupling of perception and action is familiar to owners of hunting dogs, for whom looking at an object automatically triggers movement toward it. Deleuze was not the only theorist to describe the tight link between what we see and our bodily responses. Maurice Merleau-Ponty, in <i>Phenomenology of Perception</i>, noted that perception is not merely a matter of sight; it is a process involving the entire body, and watching a film in a cinema is hardly an exception. Christian Metz, by contrast, described the cinematic spectator as a “half asleep, half awake” disembodied being.</p>
                <p>Contemporary research confirms the insights of Deleuze and Merleau-Ponty regarding the bodily nature of perception and cognition. To experience this firsthand, without delving into scientific literature, it is enough to spend a few minutes in a virtual reality headset, where the entire body truly participates in visual perception. A discovery I made thanks to my reading glasses: in a 3D headset there is no real perspective — it's just a kind of screen, and all objects, whether near or far, are actually at the same distance. Yet by taking a few steps, we can perceive the approach of objects placed at a false depth. Narrative films created in this technology function as “empathy machines,” cinema on steroids, amplifying the physiological response that in a traditional theater manifests only as a subtle vibrations. This is part of why most viewers find it difficult to spend extended periods in a VR headset.</p>
                <p>Deleuze notes that for the human spectator, movement on the screen must appear “normal.” Yet certain aberrations have always been part of cinema: fast and slow motion, reverse filming, jump cuts — brief temporal skips that omit small phases of movement — and constant changes in scale and proportion. These are what Deleuze calls “pseudo-synchronized movements,” concealing their internal contradictions.</p>
                <p>Throughout its history, cinema has tamed aberrations through editing and sensorimotor schemas — the very link between vision and action that structures our behavior in the physical world (see an obstacle, go around it). Editing created gaps between shots that allowed viewers to perceive, from their own experience, the connection between what characters see and how they act, integrating even the strangest movements into a coherent sequence. According to Deleuze, in postwar cinema,for instance, among the Italian neorealists, the sensorimotor schema is disrupted: perception and action no longer align in chains, spaces fail to coordinate with each other, and characters, captured within optical and sonic situations, are rendered to aimless wandering. Time detaches from its loops, disconnects from movement, and aberration becomes significant in itself, marking time as its immediate cause. In “slow cinema,” from Tarkovsky to Tsai Ming-liang, editing loses its central role, and the work with the time-image, which replaces the movement-image, is structured through long takes.</p>
                <p>Generated video does not simply disrupt sensorimotor schemas — it collapses them. The viewer cannot detect a stable perception-action link because there are no perceiving subjects, no purposeful actions, and no obstacles to overcome. Neural kittens pass through the bars of a basket, unaware that such a thing is impossible in the physical world. Witnessing this produces a disconcerting astonishment, similar to what the author of the Telegram channel <i>soloveev: life on Mars</i> described in the previous chapter: attempts to plan one’s own path through the basket bars fail. Comparable feelings arose in 1991 when the liquid Terminator in James Cameron’s second film transformed into anything imaginable. Predicting the next frame of generative video using familiar physical laws is impossible — this draws the viewer in, but also produces vertigo and nausea.</p>
                <p>In Deleuze, the viewer identifies with the image on the screen, trying on the possibility of the movement being observed. Lacan offers another perspective on this territory. His theory of the “mirror stage,” in which a child first sees their reflection and forms a sense of a coherent “I,” has often been applied to cinema — a kind of identification machine that offers the viewer something far more compelling than mere reflection: the chance to see oneself in the attractive, unified images of film characters. Yet, both in the mirror and on the screen, this unity remains illusory, and the fear of disintegration, the return to a fragmented self-perception, can provoke aggression. Generative video may not function as a mirror. If we are not deceived about its nature — as some naive internet users might be — we know that there is no coherent image behind it, only fragments of previous images and code. Perhaps this is one reason generative videos provoke frustration, yet we watch them in fascination, futilely attempting to process the image as if it were a cast of reality.</p>
                <p>Video generation, Claude summarizes in our dialogue about the movement-image in Deleuze, is cinema after the collapse of the sensormotor schema, where aberrant movement is no longer an exception but the rule. The link between the image and our potential activity breaks down, forcing the brain to plan actions for an unreal world in which familiar physical relations no longer hold. How far could this need to plan carry us, if the flow of such videos on social media becomes increasingly dense? Could it recalibrate our perception, alter our criteria for what is real? I think, at least partially, it could. Just as Eisenstein’s montage once seemed a radical violation of natural perception but later became a foundation of cinematic language, the distorted movements generated by neural networks may enter our visual vocabulary as an alternative way of experiencing an image. As part of a new contract between screen and spectator. What if future generations, now watching world leaders transform into predatory animals or birds, begin to perceive stable objects as outdated and boring, just as most of us today view black-and-white films, and see normal movement as monotonous and sluggish?</p>

                  <p></p>
                <p>***</p>
                <p></p>
                <p>Interestingly, long before the advent of neural network latent spaces — which generate an endless context for fragments that unfold before our eyes during video generation — Deleuze described a similar domain in cinema through the problem of framing and off-screen space. He treated the framed image as a closed system, while the off-screen space existed alongside or around it. This off-screen space functions as a disturbing “other” beyond the homogeneous time and space within the frame, adding a sense of extension — a kind of potentiality that affects what is visible and within which duration unfolds.</p>
                <p>The latent space of generative models functions in a strikingly similar way: it is an infinitely extending mathematical field where all possible image or video variations coexist in an unrealized state. Both Deleuze’s off-screen space in cinema and the latent space of models are spaces of the virtual. Neither can be seen directly, yet both shape the structure of the visible — though the former is tied to the real duration of lived time, while the latter is governed by the statistical parameters of the neural network.</p>
                <p>Long before the advent of machine vision, Deleuze essentially describes the visible space of the frame as a dataset, as an informational structure: “We will call the determination of a closed system, a relatively closed system which includes everything which is present in the image - sets, characters and props -framing. The frame therefore forms a set which has a great number of parts, that is of elements, which themselves form sub-sets. It can be broken down. Obviously these parts are themselves in image.” In both photography and cinema, framing is a conscious selection of various elements that together form a relatively closed and artificially constructed multiplicity. The closed system conditioned by the frame can be considered in relation to the data it conveys to viewers: it is informational, and can be either dense or sparse.</p>
                <p>When Deleuze talks about the frame as a closed system that includes everything present in the image, he is essentially describing the principle of dataset formation — a limited set of elements that can be “counted in detail.” His use of the term “data” is remarkable for the mid-1980s, intuitively pointing to the informational nature of the image. Interestingly, the very process of creating a dataset involves framing: the optimal image size is 512×512 (for SD 1.5/2.1) or 1024×1024 (for SDXL), and if other proportions are used, the model automatically crops them to a square format. In other words, it performs the framing for you, discarding not what is unimportant, but what does not fit into the ideal square.</p>
                  <p></p>
                <p>***</p>
                <p></p>
                <p>As Deleuze reminds us, Eisenstein repeatedly emphasized that montage is the whole of the film — its Idea. Attempts to cut and splice film appeared almost immediately after the invention of cinema in the late 19th century. Later, following the pioneering experiments of D.W. Griffith, montage became a formative element of the new medium. For Deleuze, montage reveals the artificiality of cinema, becoming a site of temporal rupture where a leap occurs, generating new meaning through the collision of heterogeneous elements.</p>
                <p>Today, video-generation models are capable of producing short clips, initially 5–10 seconds long, and more recently around 20 seconds, with durations likely to increase over time. When constructing longer, coherent narratives, these clips inevitably undergo a form of editing. Yet what happens within the video-generation models themselves, in a sense, uproots a cornerstone of cinema. The ability to merge two or more static images into continuous motion creates an effect of anti-montage, in which any rupture is masked by smooth, often paradoxical morphing. By generating these unexpected connections, the model calculates which elements — say, the boy’s clothing — facilitate his rapid transformation into, for example, a fish. The morphing of the boy into a fish is not a metaphor or symbolic association, as in Eisenstein’s Strike, where a fox or an owl became agents of the secret police with corresponding labels. Instead, it is a direct rhizomatic connection through statistical correlations. The logic of physical causality is replaced here by the logic of algorithmic mappings, where any element can fluidly transform into any other based on statistical proximity in latent space. For the model, morphing is taken as self-evident, completely natural, but for us it represents an impossible transition between incompatible states — of course, only when the user requests the combination of two genuinely incompatible images. In professional content creation, frames are carefully chosen so that their combination appears convincing within human logic. One could say that this anti-montage occurs not in time, as in cinema, but in the Schrödinger-like space of infinite statistical possibilities.</p>
                <div class="note">
   Of course, the model takes context into account, and it is more likely to turn a diving boy into a fish than a running one, but the chance of an illogical transformation during generation still remains high.
</div>
                <p>Deleuze quotes Pasolini, who noted that montage has the ability to transform our unstable present into a “clear, stable, and describable past,” in other words, to bring time to a close — and death does the same. Generated videos, however, are images not tied to events in material reality; they have neither past nor future. Each frame exists in the continuous “now” of generation.</p>
              
            </div>
        </section>

        <!-- CHAPTER 7 -->
        <section id="chapter7" class="chapter">
            <h2 class="chapter-title">Chapter 7. Yes, I Love You! How We Got Used to Vertical Images</h2>
            <div class="chapter-content">
                <p>Sarah can’t decide between her modest childhood friend and a charismatic classmate who has already raised his hand against her. Two sisters — one “good,” one “evil” — enter a brutal competition over their father’s inheritance. Nicola has given birth to a child by a young politician, but he never found out, because his mother tore them apart. A princess is locked in a dungeon, and the new ruler who killed her father orders her to come to his chambers at night. The interiors and faces look like they came from stock photos, the background music never stops, the action unfolds slowly, each episode lasts a couple of minutes, and the total running time is about two hours. The camera mostly lingers on close-ups. Time inside the story moves in jumps — we only learn that months or years have passed between episodes through the dialogue. Welcome to a new era in which human dramas unfold literally in the palm of your hand.</p>
        
                <p>“Vertical dramas,” shot in the 9:16 format specifically for smartphones, first appeared in China in the mid-2000s. In the early years, they were often produced by small or semi-legal studios, but by the late 2010s, major internet companies like Tencent, iQIYI, and Douyin began actively investing in the business. Anyone who has come across Chinese vertical series on TikTok, Instagram Reels, or in specialized apps — DreamShot, SturdustTV, ReelShort — notes their absurdly low production quality and incredible addictiveness. Within the attention economy, where an excess of content turns user time into a scarce resource, a new toxic flower has bloomed. Journalist Yunia Pugacheva, who lives in Los Angeles, <a href="https://t.me/yunapugacheva/2259" target="_blank">described</a> her first encounter with a Chinese vertical drama — about a billionaire being given a new girlfriend to replace the one who lost her memory after a car accident — like this: “I was watching and thinking: this is terrible. But I couldn’t stop. Each episode ended at such an unexpected moment that my hand automatically reached to play the next one.”</p>
                
                <p>The pleasure doesn’t come cheap: a one-week subscription to the DreamShort app costs 1,790 RUR per week (available for purchase in Russia through the App Store), and an annual plan comes to almost 20,000 RUR. If you don’t want to spend money, you can earn bonuses by watching ads — but for two minutes of your favorite show, you’ll have to spend half an hour turning into a click machine, plunging deep into the world of chaotic traffic aggregators filled with generated images and text.</p>
                
                <p>At the dawn of cinema, silent films were often remade from the same script in different countries with different actors. Today, Chinese dramas reach international audiences in much the same way — adapted and reshot with non-Asian actors, such as Teig Sadhana, who used to walk dogs in Melbourne and now earns $1,000 per shooting day, enough to live comfortably in New York. Blanca Schofield, a correspondent for <i>Air Mail</i> who wrote about Teig’s story, <a href="https://airmail.news/issues/2025-7-19/cliff-hangers-for-the-commute" target="_blank">describes</a> the main parameters of this growing phenomenon: filming one series takes 8 to 10 days; actors who would never make it into mainstream film or television get a chance to earn money and enjoy their fifteen minutes of fame; the budget for each drama is around $150,000, and the scripts are usually written by Chinese writers. For Russian viewers, the dialogue may resemble the now-iconic exchange from the low-budget TV melodrama <a href="https://www.youtube.com/watch?v=3IEujakzEqM" target="_blank"><i>Apple Orchard</i></a>, which became a meme on the Russian internet: “She turned me down. She said she loves you. She <i>thinks</i> she loves you! — She can’t love me! — But I love you!” The titles sound like something from late-night erotic programming on REN-TV: <i>Captured and Bound by My CEO</i>, <i>My Wife’s Lover Is the Chauffeur</i>, <i>Tied to the Forbidden Alpha</i>, <i>The Maid’s Hidden Agenda</i>, <i>Prove Your Innocence Before It’s Too Late</i>.
</p>
               
                <p>In China, revenue from vertical dramas in 2024 surpassed box office earnings, reaching $6.9 billion. Worldwide, the figure amounted to $1.2 billion, with 60% coming from the United States — mostly through the ReelShort app, headquartered in Silicon Valley. In the post-pandemic downturn, these low-cost micro-dramas proved to be an effective way to reinvigorate the entertainment industry. Insiders familiar with the production process in Russia say it usually involves a long chain of subcontractors, reminiscent of the invisible networks of intermediaries described in <i>The Atlas of AI</i> — the same kind that facilitate the global movement of physical goods like microchips, and are nearly impossible to trace due to logistical complexity.</p>

 <p></p>
                <p>***</p>
                <p></p>
          
                <p>In their book <i>Film Theory: An Introduction Through the Senses</i>, Thomas Elsaesser and Malte Hagener note that while a film itself is intuitively associated with the idea of a window — something we look through, like Hitchcock’s protagonist in <i>Rear Window</i> — the poster often serves as the point of entry, a kind of visual concentrate shaped like a door. A vertical, street-level advertising stand is physically closer to pedestrians than the large horizontal billboards towering above the city on the upper floors of buildings. In this sense, the horizontal format feels authoritative and exclusive, while the vertical one appears more democratic and inviting.</p>
            
                <p>Why did cinema become horizontal in the first place — and why is it turning vertical now? When engineer William Dickson, working in Thomas Edison’s laboratory, was developing the first motion picture cameras, he wanted the image on the film to be clear and roughly one inch wide by three-quarters of an inch high — a 4:3 aspect ratio. At the time, photography both inherited from and competed with European painting, where the horizontal orientation had been dominant for centuries — it suited human vision and fit naturally into the interiors of large houses and palaces where such works were displayed. As a result, Eastman’s standard photographic film was 70 mm wide. Dickson cut it in half, producing a strip roughly 35 mm wide, minus the perforations on both edges.This gave a frame size of about one inch by three-quarters of an inch — still horizontal, with the width exceeding the height. Other cinema pioneers, such as the Lumière brothers, quickly adopted the format. It proved convenient and practical for filming, processing, and projection, while also aligning with the way the human eye naturally perceives the world.</p>
                   <p>After centuries of painting, cinema, and photography, the agreement between the viewer and the horizontal screen had become so strong that it took something as simple as a generational shift to change it. “It was probably over ten years ago now, but I can still remember my reaction the first time I saw someone shoot a video clip with their phone in the vertical orientation. I had to resist the urge to tap them on the shoulder and point out their error,” <a href="https://amateurphotographer.com/latest/opinion/why-i-hate-vertical-video/" target="_blank">wrote</a> one of the contributors to the <i>Amateur Photographer</i> website in 2024. “As I started to see more people doing this, I became irritated that people were just too lazy to turn their phones 90 degrees. Now I’m just resigned to the fact that a whole generation are doing video wrong.”
</p>
                
                <p>Long-time users will remember that, in its early days, Instagram* (banned in Russia) only allowed square images — perfect little frames, like ideal pictures from a dataset. Later, the option to change the aspect ratio appeared, and Snapchat, launched in 2011, became the first major platform for vertical content. In the second half of the 2010s, with the rise of China’s TikTok and Instagram* Stories, vertical photos and videos gradually became the norm. Like the author of <i>Amateur Photographer</i>, I used to feel uneasy about the “wrongly” rotated triangle narrowing the horizon — until I saw that everyone was doing it and realized there was no way back. A principled refusal to post vertical images soon turned into a refusal of social contact itself, since vertical “Stories” at the top of the app compensate for the arbitrariness of algorithmic feeds. They allow users to curate their own list of interesting accounts and watch them in chronological order. (Once upon a time, you could scroll to the end of your feed and see a message saying there was no new content left — but that went against the insatiable demands of the “attention economy.”) Over time, Instagram* Reels, YouTube Stories, VK’s “Istorii” and, of course, the trendsetting space of TikTok trained humanity — by 2024, 60% of the world’s population used smartphones — to consume vertical videos. A habit shaped by decades of cinema and the physiology of vision reluctantly gave way to the physiology of the human hand gripping a smartphone. In the end, “democratization” turned out to mean the right not to make an extra movement — not to rotate the phone.</p>
                 <p></p>
                <p>***</p>
                <p></p>
                
                <p>Cinema is a stubborn form, and the most striking example of its resistance to verticalization is the sudden change of aspect ratio in Xavier Dolan’s <i>Mommy</i>, shown in competition at the Cannes Film Festival in 2014. At first, the fifth film by the Canadian wunderkind and visionary seems to be shot in a square 1:1 format, which at the time was associated with Instagram*. However, about halfway through the movie, the main character pushes apart the frame with his hands, as if expanding his own horizon to a normal widescreen ratio — a moment that drew a collective gasp of admiration from the audience in Cannes’s Lumière theater.</p>
                  <p>Unlike social media, the move toward vertical cinema has been gradual, and the process is still far from complete. If one wishes, the genealogy of vertical films can even be traced back to D. W. Griffith, who often used black bars along the edges of the frame to obscure parts of the image and draw attention to specific actions. Eisenstein, in turn, spoke about the importance of vertical composition in cinema, referring to Japanese and Chinese scroll paintings. Some feminist scholars interpret the vertical format as another possible way to break free from cinema’s original curse — the “male gaze.” Yet it was the smartphone that gave rise to the phenomenon of “upskirting,” the unwanted intrusion of the camera and the gaze beneath women’s skirts in public spaces.</p>
                
                <p>In recent years, there have been several attempts to make films in a “phone” aspect ratio. In the mid-2000s, a number of vertical film festivals appeared in different countries, where participants explored the creative potential of this new format. The 2021 Russian film <i>Devyataev</i>, about a Soviet pilot, was originally planned as Timur Bekmambetov’s vertical experiment, but the creators eventually abandoned the idea. In 2020, Damien Chazelle, the director of <i>La La Land</i> and <i>Whiplash</i>, shot a 10-minute film called <i><a href="https://www.youtube.com/watch?v=5f1WymwaOVo" target="_blank">The Stunt Double</a></i> on an iPhone — a story about a stuntman who, after falling from a great height, finds himself moving through different genres of classic Hollywood cinema, from silent films and westerns to action movies. In some scenes, Chazelle makes clever use of “negative space,” placing the characters at the bottom of the frame so they appear small against the backdrop of nature; in others, the cropped faces and eyes of the protagonist look awkward. Still, it’s clear that his work is an ideological attempt to fit traditional cinema into the vertical screen of a smartphone. The problem is that it simply doesn’t fit — and all of film theory, starting with Gilles Deleuze, was written in the 20th century entirely for horizontal cinema.</p>
                
                <p>In the traditional horizontal frame analyzed by Deleuze, <span style="background-color: #abc4c5; padding: 2px 4px;">space is organized like a theatrical stage — the viewer observes the action from a certain distance, taking in a wide panorama. This allows for complex compositions with multiple layers, which the philosopher insightfully compared to “data sets.” The shift to the vertical format radically transforms Deleuze’s concept of framing, creating an entirely different logic of visual perception.</span> In a vertical series, the frame contains almost nothing except the faces and bodies of the main characters. The dialogue is written so that one can follow the action half-attentively, mainly through sound, much like a television soap opera — except the screen is too close to the body and demands constant background engagement. The set of visual data on the screen is so minimal that the viewer clings gratefully to any small detail — for example, a portrait of a young Angelina Jolie with a cigarette, hanging for some reason in the room of a student in love with his indecisive childhood female friend.</p>
                  <p>However, the vertical format has its own advantages. It creates an intimate space that relies not on the amount of visual information but on emotional resonance — on the instant and short-term capture of attention. A vertical film is a mirror-film, a nomadic film that is always with you: individualized, fast both in production and in its entire life cycle. Philosophically, this connects to the work of Gilles Deleuze and Félix Guattari, who described affect as a force that operates before and independently of consciousness. Affect is not an emotion we experience, but an intensity that grips and transforms us before we even understand what is happening. Vertical series work precisely on this level — they bypass critical thinking and act directly on the body, on automatic reactions. When a journalist writes, “My hand just reached out to play the next episode,” she perfectly describes the logic of affect: the body already knows what to do while the mind still resists. Chinese dramas are the perfect affective machine: every cliffhanger is a jolt that makes the body demand more. Once, the suspense at the end of an episode made viewers return to their TV screens again and again, but now attention is captured not in 30–50 minute intervals, but through short two-minute cycles. Post-cinematic media, as Steven Shaviro describes, act directly on the nervous system through rhythm, intensity, and sensory stimulation, transforming the nature of affective engagement once familiar from films and television, and creating a fragmented temporality — for instance, those unexplained gaps of months or even years that “verticals” present to us without comment. Microdramas, with their unprecedented proximity to the viewer, take the next step in colonizing our time within the “attention economy.” Not everyone can follow the tangled emotional plots, but for those who are susceptible, these narratives devour every pause — on the subway, in line, during a lunch break — converting personal time into profit by seizing affect. On one hand, this is nothing new — short videos on social media do the same — but on the other, as in cinema of the past, attention is captured not through rapid image changes but through an unusually slow, sequential narrative.</p>
                  <p>

For now, vertical microdramas, often made on the fly, resist traditional critical analysis: hierarchies have not yet formed, there are no clear points of reference, no globally recognized stars. Trying to decide what to watch can feel like a small panic attack, like the tremor of a minor earthquake. Yet even a superficial look at media evolution suggests that every new format tends to grow from simple to sophisticated, even in mass culture. As noted earlier, today any “reels-maker” possesses skills that once belonged only to professional filmmakers. Horizontal cinema is unlikely to disappear anytime soon — it will simply lose another portion of the audience’s attention, yielding it to the forbidden Alpha, the maid hiding her intentions, and those new heroes of vertical storytelling who have yet to appear.
</p>
                
            </div>
        </section>

        <!-- CHAPTER 8 -->
        <section id="chapter8" class="chapter">
            <h2 class="chapter-title">Chapter 8. Transit Zone. From Digital Photography to In-Game Documentary</h2>
            <div class="chapter-content">
                <p>“But that’s not even a photograph!” my companion says as I tell her about the Google Street View screenshots mentioned in the next chapter of this book. We’re waiting for the opening of a new exhibition at the gallery <i>Transit Zone</i> — a small space that used to be a bus stop on a highway in the resort district outside of Saint Petersburg. In a week, a noise performance will take place here, with the audience watching from the street through the glass, mingling with pedestrians, cyclists, and roller skiers.</p>
                <p>Yes, a digital image is not a real photograph — but what is a photograph, anyway? The twentieth century offered many possible answers. Susan Sontag saw photography as a specific form of control over the past — both a keeper of memory and a means of transforming it. Vilém Flusser viewed the photographer as an operator executing the program of the camera, functionally close to today’s prompt engineer interacting with an algorithmic model. For Walter Benjamin, photography symbolized the era in which the technical reproducibility of artworks changed the very notion of art itself. French writer and filmmaker Jean-Louis Comolli suggested that with the advent of photography, the human eye lost its “eternal privilege,” devalued in comparison to the “mechanical eye” of the camera. Roland Barthes understood photography as a mythological system capable of creating new meanings. For the replicant Rachael in <i>Blade Runner</i>, unaware of her artificial nature, childhood snapshots served as undeniable proof that she was human. Many theorists agreed that photography produces meaning not through the relationship between signified and signifier, as in language, but through a direct physical connection between the image and the object of the material world, which affects the photosensitive medium through reflected or emitted light. Today, we mostly call a photograph an image taken on a smartphone, quickly forgetting that brief period in the history of technology when it was called “mobilography.” And yet, “this is not even a photograph,” because the algorithmic processing in modern digital devices automatically severs the link between object and image that once underpinned the photochemical process. In contemporary smartphones and cameras, shooting can no longer be separated from post-processing. It is essentially one and the same operation: <span style="background-color: #abc4c5; padding: 2px 4px;">sensors capture light</span>, convert it into an electrical signal, assign numerical values to it, <span style="background-color: #abc4c5; padding: 2px 4px;">and built-in processors apply algorithms to enhance the image — adjusting exposure, reducing noise, balancing white, sharpening, and more</span>. In the analog era, these were the tasks of the retoucher — and while in the early days, the same person handled shooting, printing, and correction, by the mid-twentieth century, the labor had been divided, and retouching, especially for the printed press, became the job of trained specialists.
</p>
                <p>One way or another, every selfie you take is already algorithmically processed — it already bears the imprint of "artificial intelligence".</p>
                
                <p>But was real, analog, printed photography ever a true and complete representation of reality? Of course not. In its early decades, people had to pose for long periods in stiff, unnatural positions. Then it lacked color. It never had movement or sound. The lens distorts the image, and framing creates a hierarchy of reality, deciding what deserves to be seen and what does not — perhaps that’s why zoomers, with their more horizontal perception of the world, are more likely to photograph and post a price tag from a grocery store or a dead pigeon near a dumpster than a conventionally beautiful flower or a famous landmark. “Documentary” is a highly problematic notion: the presence of a camera transforms reality, and staged scenes are often passed off as spontaneous. The famous <i>Napalm Girl</i> photograph from Vietnam was most likely staged. Greta Zimmer Friedman, the woman believed to be in the <i>V-J Day Kiss</i> photo from Times Square, later said that she had experienced a violent touch from a stranger — a devastating blow to the romantic myth labeled as “reality” (though it’s not even certain that it was her in the picture). One might say that the “documentary” doesn’t truly exist — only a kind of contractual frame with the viewer does, an agreement to accept the label as a certificate of authenticity, sometimes even in ideological opposition to the fictional, which is seen as staged and therefore “unreal.”</p>
                <p>Today, to see real photographs, we go to museums, while family albums are being digitized — a paradoxical act of voluntarily translating material memory into a less stable state, since analog archives are still considered more reliable than digital ones. What has been digitized, like dreams, cannot truly be possessed in the usual sense. Yet on an imaginary scale similar to the one proposed by theorists of the “uncanny valley,” a paper photograph still lies closer to the authentic. That’s why manipulations such as the radical retouching of the <a href="https://en.wikipedia.org/wiki/The_Commissar_Vanishes" target="_blank">“vanishing commissars”</a> provoke outrage, fear, and shock — as if reality itself had been violated. A computational image taken with a smartphone is still considered a photograph, though a less valuable one, while an AI-generated image posing as a photograph is likely to be labeled a disgusting fake, unworthy of attention. All of this seems less like an attempt to restore a genuine connection between the object and its representation, and more like a complex system of agreements that are renegotiated at each new stage of technological development — “Do you keep the copies of old agreenents?”
</p>
             
                <p>With the arrival of digital technology, analog photography was pushed off the forefront, just as over a century earlier it had itself pushed painting to the periphery. In the 19th century, artists and poets greeted the emergence of photography with the same curses that today illustrators, working not with easels and paints but with computer software, direct at generative models. Charles Baudelaire, in his essay <i>The Salon of 1858. The Modern Public and Photography</i>, condemned the recent invention as “a refuge for failed artists, poorly gifted or too lazy apprentices, and consequently the widespread enthusiasm for it not only acquired the character of blindness and folly, but was tinged with a certain malice.” Baudelaire linked the success of this new medium, whose messiah was a certain “Engineer Daguerre” (engineer — that is, anti-artist), to “the stupidity of the crowd,” which expected complete lifelikeness from art and was unable to accept creatively reimagined reality. Both the poet himself and many after him somehow accepted on faith (signed a contract) that photography, whether good or bad, was proclaimed an exact imprint of reality.
</p>
            
                <p>The concerns were not unfounded, and in a sense photography did indeed displace painting, taking away a number of monetizable functions — from portraiture to the preservation of memories. Before ultimately becoming an archival format, painting responded with a creative surge of artistic activity at the turn of the 19th and 20th centuries — and it seems this happens with any outdated medium that refuses to give in without a fight. Nevertheless, many artists of that era were fascinated by photography, including Edgar Degas, who in his later years took photos in intimate, dimly lit settings, and Edvard Munch, whose blurred shots transform bodies into shadows.</p>
              
                <p>Today we find ourselves in a unique situation, where the penetration of new technology takes place on the global agora of social networks, and a new contract between image and viewer is being formed in real time — while Instagram (banned in Russia) fills with AI-generated brain rot, and Twitter (X) users laugh at older Facebook (banned in Russia) users, who often take obvious fakes at face value. There seem to be two simultaneous modes of perceiving this new type of image. For those unfamiliar with the technology, they effortlessly cross the "uncanny valley," categorizing images as credible (after a major fire in Los Angeles, actress Isabella Rossellini, among many others, posted a photo of a charred Oscar statuette in a pile of ashes — an obvious fake, readily believed because of its symbolic power). Others are attentive enough and sufficiently tech-savvy to suspect or recognize generation and respond negatively or, conversely, positively. "Somewhere in paradise, Balabanov is filming <i>Brother 3</i> with Bodrov and Tsoi…" writes a YouTube user under a 17-second AI clip in which the deceased actor and musician relax between shoots; the audience desperately wants them to be alive, and even understanding that it is AI, they are willing to accept the result nonetheless.
                    <div class="note">
   Viktor Tsoy died in 1990, Sergey Bodrov died in 2002, Alexei Balabanov died in 2013.

</div>
</p>
          
                <p>It is intuitively clear that an image will be considered "acceptable," "authentic," and "high-quality" if it is: a) an index of a real object existing at a specific moment in the past (only the past, since it is impossible to photograph the present or the future), b) the result of collective or personal labor and/or talent. It is also obvious that a generated image is not a simulacrum, a copy of something that does not exist. Its connection to the material world exists, but at a very high level of abstraction — millions of once-photographed real objects dissolve into datasets and become the building blocks for a new representation.</p>
                 <p>Creating a paper photograph is a long, expensive, and labor-intensive process compared to producing a computational image that exists on a smartphone screen. However, in the case of the latter, the amount of labor and cost is no less — and often greater — than in manual photo printing. This labor is hidden from direct perception, dissolved in extended engineering, technological, and logistical chains, broken down into algorithmic steps, like on a Ford assembly line, where the “author of the photograph” is the Flusserian operator pressing a button on a device, the production of which involved thousands of people. Aren’t we claiming their labor for ourselves (even if it is paid for through the cost of the smartphone) just as a prompt engineer claims an unlimited number of physical objects that do not belong to them, dissolved in datasets? In the system of agreements between an image and the eye, applied to familiar digital photography, this question simply goes unnoticed.</p>
                  <p>The attack on our perception of reality comes from completely different directions. In the anthology <i>Virtual Photography. Artificial Intelligence, In-game, and Extended Reality</i>, photographer and theorist David Bate discusses IKEA’s new advertising strategy — starting from the mid-2010s, 75% of the Swedish furniture giant’s catalogs are computer-generated graphics rather than studio photography. This approach significantly reduced production costs (building, for example, a kitchen in a studio takes about two weeks) and minimized waste. The new strategy was widely covered in publications and did not affect IKEA’s reputation, whose posters and catalogs have always offered the promise of owning the best version of everyday reality — likely in no small part because a chair designed in a computer program still has a prototype in the physical world. 
                  </p>
                  <p>“There is a common misconception today that AI-generated images are a collage or mashing together of existing imagery,” writes researcher Amanda Wasielewski in her article <i>The Hidden Objective World</i>, included in the same anthology <i>Virtual Photography</i> — “Collage and remix require fragmentation and, even, repetition, but are essentially heterogenous mixtures that still maintain the integrity of their parts. AI-generated images, on the other hand, are the products of a chemical reaction where the parts that go into making it are fundamentally transformed in the process. The outcome is no longer merely a mixture or a mashing together of parts but something new.” Fragments of a collage can theoretically be traced back to their originals, but just as one cannot reconstruct firewood from ashes, one cannot reliably recreate the prototypes of a generated image. One can only outline a path, as in the attempt to discern a starting point for future generations in my photograph of director Brillante Mendoza. But do we not think in transformations rather than copies? Were generations from datasets not already conceived in the concept of assemblage, which emerged with Deleuze and Guattari largely from reflecting on their own experience of collaborative writing — a multi-layered, heterogeneous combination creating new qualities? Does not the same metaphor of a chemical reaction describe what happens in the minds of postmodernist directors like Quentin Tarantino and Paul Thomas Anderson, whose works are dense palimpsests of others’ influences while remaining strikingly original, whereas the natural “datasets” of hundreds of other films that formed their style are scrutinized by fans? Can anyone truly claim something as their own after reading Thomas Buddenbrock’s inner monologue, suddenly realizing himself as the starting point of all pursuits and vocations, and who will, even after death, reside in all who have ever spoken, speak, or will speak? Did Thomas Mann not anticipate the advent of LLMs in the finale of his novel? And can anyone besides me, with my individual dataset in my mind, draw a parallel between Thomas Buddenbrock’s deathbed insight and large language models?</p>
                  <div class="note">
   Interestingly, the platform The Simulation, which allows viewers to change the faces of film characters and storylines using AI, promotes its product on Twitter through a reference to postmodern literature, posting the cover of Tom Stoppard’s play <i>Rosencrantz and Guildenstern Are Dead</i>: “The story of Hamlet in a POV version from NPCs (a term from video games, referring to non-player characters who essentially serve as set dressing), an AI remix of Shakespeare made by a human.”


</div>
                   <p>Postmodernism in late 20th-century art was an analog way of generating something new from what already existed — it gave humanity time to prepare, anticipating by several decades the machine-driven manifestation of everything that had become everything.</p>
                     <p></p>
                <p>***</p>
                <p></p>

                    <p>“We are being attacked by small screens,” said Marco Müller, the former artistic director of the Venice Film Festival. In the late 2000s, he embarked on a bold but knowingly doomed experiment, turning the festival’s secondary competition, usually reserved for lesser films, into a platform for explorations at the intersection of cinema and media art. Among the participants and laureates of the program were the young Canadian visionary David O’Reilly, for whom the 8-bit aesthetic of early computers was already a subject of nostalgia, and the Austrian artist Peter Tscherkassky, who worked with film stock as a sensitive archival medium.</p>
                     <p>Cinema is a conservative medium, and it long resisted the intrusion of small glowing screens onto the big screen. In many horror films, the action is still either deliberately set in the pre-digital era, or mobile phones are turned off at the very beginning to preserve genre conventions.We are attacked by small screens, and in the early decades of mobile communication, we were attacked by screens filled with text. One of the first pop culture projects to use short messages as an element of action was the TV series <i>Sherlock</i> (2010–2017), whose creators extracted the content of smartphone screens and incorporated it into the “dataset” of the cinematic frame. By placing a smartphone screen within the frame in <i>Personal Shopper</i>, Olivier Assayas managed to create suspense, a detective story without a solution, in which the protagonist was pursued not by a person, but by text she could not simply turn off, because then deadly silence would follow.</p>
                      <p>An even bolder intrusion of digital reality into the physical occurs in <i>Videophilia (and Other Viral Syndromes)</i> (2015) by Peruvian director Juan Daniel F. Molero. The protagonists of this film, which won one of the awards at the Rotterdam Festival, are a schoolgirl who seeks adventures in the digital world, taking it for granted as something ordinary, and a young man trying to make money shooting pornography, but failing due to his overly artistic approach. This little-known film, from which almost the entire audience walked out when I screened it at the Erarta Museum during our media archaeology conference in 2015, impresses with the audacity with which the director dissolves network artifacts into the atmosphere of a nighttime city; Molero does not embed the screen within a screen, he simply releases glitches onto the streets, and animated GIFs flicker before the eyes of both the characters and the viewers, just as they flicker before the eyes of those who stare at a monitor for too long.</p>
                      <p>Today, a similar visual experience is easy to reproduce by launching an augmented reality app — for example, Acute App, which allows users to place and view works by artists like Olafur Eliasson or Koo Jeong-A within a real landscape or interior through a smartphone screen. With the advent of cloud platforms for developing AR objects, AR interventions have become a common artistic practice, challenging perceptions of reality in a situation where people look at their smartphones almost as much as they dream. Any QR code stuck on a fence can lead to a provocative 3D statement.</p>
                       <div style="text-align: center; margin: 20px 0;">
                <img src="room.jpg" alt="SITTING IN A ROOM. I AM by Viktor Vilisov" style="max-width:100%;">
                <div style="font-style: italic; font-size: 0.85em; margin-top: 5px;">
                Ill. 5. A fragment of Victor Vilisov’s AR performance SITTING IN A ROOM. I AM. and my hand.
               
               </div>
               </div>
                     <p>The intrusion of foreign objects into a familiar environment (for example, one’s own apartment) creates an unexpectedly strong impression upon first encounter. At the end of 2020, in the midst of the pandemic, the AR performance <i>SITTING IN A ROOM. I AM.</i> became available on iOS and Android — an intense show for a single viewer in a confined space, created almost entirely by the theater critic, writer, and later artist <a href="https://vlsv.gay/" target="_blank">Victor Vilisov</a>. Drawing on the story of a British unemployed man who, in that same COVID year, died of starvation after being denied benefits, the project leads the viewer to realize that unconditional basic income is not a fantasy or illusion, but a natural necessity in a world where most existing jobs will, within our lifetime, be taken over by robotics. There is hardly a more suitable way to imagine the hypothetical than a performance using elements of augmented reality. <i>SITTING IN A ROOM. I AM.</i> is a mobile app, and to experience the 80-minute show, all that is needed is a charged smartphone, headphones, a piece of floor or any other surface, and some natural or artificial light. The performance alternates between animation on the phone screen and AR objects in the room, which the viewer must move around to observe from all angles, while voices in the headphones discuss different approaches to the problem of unconditional basic income. The impulsive habit of staring at the phone is temporarily suspended during interaction with this show, since the viewer is already looking at the phone. The combination of a familiar environment and vivid foreign objects is so invasive that after turning off the screen at the end of the performance, I still couldn’t understand why the little sheep were no longer in my room.</p>
                      <p>Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p>

                  <p>Although Elsaesser and Hagerer lament in their book that a physiology of aesthetics still does not exist, this visual phenomenon has scientifically grounded explanations. The digital hallucinations of the characters in <i>Videophilia</i> and my longing for the little sheep are not fantasies, but physiological reactions of the brain to the content of intrusive small screens. In 2019, a research group from several American universities published an article in <i>Nature Human Behaviour</i> showing that intensive interaction in childhood with objects from the game <i>Pokémon</i> — zoomorpho-anthropomorphic entities that barely resemble real-world objects — produces long-term changes in the brain’s visual system. Adults who grew up on this game underwent fMRI scans, which revealed, when Pokémon images were shown, specific activation in the temporal lobe associated with face and object recognition, indicating the formation of a separate neural pattern for these objects. Pokémon were literally imprinted in the players’ consciousness — just as the faces of celebrities, such as actress Jennifer Aniston, become imprinted: a neuron responding to her image was discovered in 2005, just when the first rumors of Aniston’s breakup with Brad Pitt emerged, as he became involved with his co-star from <i>Mr. & Mrs. Smith</i>, Angelina Jolie. Today, surreal creations of generative models are imprinted in the structures of our brains in exactly the same way.
</p>
<p></p>
                <p>***</p>
                <p></p>
                     <p>The elasticity of our agreement with the “documentary” became especially noticeable in the 2000s with the emergence of a phenomenon known as “documentary animation” — an absolute oxymoron that was nevertheless enthusiastically received, at least by festival audiences. The film <i>Waltz with Bashir</i> (2008), shown in the Cannes competition, reconstructs events from Israeli director Ari Folman’s military service during the 1982 Lebanon War, some of which he had partially forgotten due to psychological trauma. The foundation of this animated film consisted of genuine interviews with former soldiers and other witnesses, which allowed it to be classified as a documentary. In another documentary, <i>The Missing Picture</i> (2013), director Rithy Panh tells the story of his family, who survived the genocide in Cambodia under the Khmer Rouge. Almost no photographs or records from that period remain — Panh reconstructs nonexistent documents using clay figurines that “play the role” of his relatives and other Cambodians subjected to deportations and violence. The artistic approach is so powerful that even years later, <i>The Missing Picture</i> is remembered as a documentary testimony. Later, the technique of animation in documentary filmmaking, tracing back to documentary graphic novels such as Marjane Satrapi’s <i>Persepolis</i> (2000), was revisited multiple times, not only in traditional cinema. The VR experience <i>On the Morning You Wake Up (to the End of the World)</i> tells, in a similar — though, due to the technology, far more invasive — manner, the story of how on January 13, 2018, residents of the Hawaiian archipelago received a false alert of a massive missile attack and spent several hours awaiting the apocalypse. Viewers are literally inside this nightmare and become witnesses to the events, even though they are surrounded only by three-dimensional animation. It turned out that documentary cinema no longer requires a document as a verified imprint of physical reality, and moreover — physical reality itself is no longer necessary.
</p>
                      <p><i>We Met in Virtual Reality</i> (2022) by Joe Hunting — probably the first feature-length documentary in history entirely filmed in a digital environment. The space in which several romantic storylines unfold is VRChat — a hybrid of a gaming platform and a social network, where enthusiasts and 3D modeling professionals can create their own “worlds” — separate virtual spaces that users enter through a menu from a shared “room,” resembling a hotel lobby. A “world” can be a cinema showing your favorite YouTube videos on a big screen, or a platform suspended in zero gravity with numerous hidden caves. The most famous Russian-speaking “world” is a reconstruction of a courtyard in a residential neighborhood with a Pyaterochka store, a gathering point for dozens of users hidden behind unusual avatars (a <a href="https://www.youtube.com/watch?v=p6fp2oZI2OI&t=35s" target="_blank">short documentary</a> about this phenomenon is also available on YouTube). Although a VR headset is not required to explore the “worlds” — VRChat can be downloaded on a computer through the Steam gaming service and explored on a regular screen — specialized equipment creates a completely different immersive experience. VRChat is a global but fairly niche and marginal community in the context of social networks; its monthly audience on Steam is approximately 40,000 users.</p>
                      <p>Hunting spent about a year on the platform, immersing himself in various subcultures and recording interviews with users. His film tells the stories of people who met, made friends, and fell in love in VRChat during the pandemic, when the physical world suddenly became less accessible than virtual reality. One of the protagonists started giving dance lessons there during quarantine, while another continued her career as a sign language teacher. The film follows several main characters, including two couples in love who are separated by hundreds of kilometers in the physical world — DustBunny and Toaster, as well as DragonHeart and IsYourBoi. These are local celebrities whose intimate stories became known to a wider audience, yet their real names and biographical details remain private, partly resolving the perennial ethical dilemma faced by documentarians or journalists who sometimes draw unwanted attention to their subjects and irreversibly change their lives.The characters on screen represent genuine, pure human existence in a digital avatar, without any reference to real names verified by state authorities.</p>
                        <p>All the main characters in the film are likely real people, enthusiasts of VR culture — their active accounts are available online, for example on Twitter (X). Yet an uninitiated viewer might take the film for computer animation: each character has a human voice but a non-human appearance, most often inspired by anime culture. The rapidly changing settings in this film visually resemble David O’Reilly’s chaotic <a href="https://vimeo.com/19723116" target="_blank"><i>The External World</i></a> or the previously mentioned Gorillaz project, only vastly expanded and more complex. Human beings living in VRChat don’t need a human appearance — not only the participants, but also the documentary’s viewers now perceive DustBunny and Toaster in the unrealistic bodies they chose for themselves. Virtual space is just as suitable as the familiar physical world for the emergence of human emotions and relationships.
                            <p></p>
                <p>***</p>
                <p></p>

</p>
                        <p>Screenshots have always been an important practice in gaming culture — gamers are used to sharing their achievements or unexpected discoveries within game worlds on social media. Many developers quickly recognized the marketing advantages of this side activity. As early as the late 1970s, Activision, which published games for the Atari 2600 — the very console standing in Russell’s room in Gorillaz — encouraged players to reach high scores by offering prizes in exchange for proof in the form of a photograph of their TV screen. The photos, mostly taken with Polaroid cameras, were usually not kept — anything related to games was considered unimportant and not worthy of archiving.Since the mid-2000s, photo modes and various types of cameras began appearing in games — now images could be captured not only by the player through a screen capture (screenshot) but also by the in-game character. With the introduction of in-game cameras, the process of capturing images in a game started to resemble real-world photography, and final images no longer included the overlay text inevitable in screenshots, allowing focus on the aesthetic aspect. Games entirely dedicated to the act of photographing emerged, such as Umurangi Generation (2020), where players took photos and discovered new equipment within the space of a dystopian city on the eve of catastrophe.</p>
                    <p>The digital equivalent of tourist photography — the computer doesn’t care, as Manovich noted, whether an image has an indexical connection to reality or is entirely man-made: for the machine, both are just pixels — immediately became a social and artistic practice. One of the earliest examples is the series of digital portraits <i>Thirteen Most Beautiful Avatars</i> by artists Eva and Franco Mattes, created in the virtual world of <i>Second Life</i> and exhibited in a New York gallery in 2007. British artist and game designer Garrett Damian Martin finds locations in video games such as <i>Grand Theft Auto V</i>, <i>Gravity Rush 2</i>, <i>Kane and Lynch 2</i>, and <i>Dog Days</i> that were not originally intended for prolonged viewing but are emotionally striking, and then re-photographs the digital images using a 35mm analog camera and black-and-white film — a process that, as we have seen, is not only a poetic reflection on the perception of the virtual but also increases the value of the images.“ Part documentary, part street photography, part cyberpunk, Damian Martin’s “heterotopias,” as they term those images, evoke an uncanny sensation of the world’s liminal zones,” writes artist and researcher Joanna Zylinska in her book <i>The Perception Machine</i>. She attended Martin’s in-game photography workshop and created several projects using this technique herself, including the <i>Flowcuts</i> series — images captured from different angles around various scenes and locations using the in-game camera function in two post-apocalyptic games, <i>The Last of Us</i> and <i>Everyone Has Gone to the Rapture</i>.
</p>
                        <p><i>We Met in Virtual Reality</i> was shot like a conventional documentary — using the VRC Lens camera, which looked like a real camera and had all the necessary functions: <span style="background-color: #abc4c5; padding: 2px 4px;">adjustable focus, zoom, depth of field, exposure, and even drone flight</span>. It was developed by a user named Hirabiki, dissatisfied with the quality of VRChat’s built-in camera. In physical reality, Hunting, who also served as the cinematographer, was in a confined space equipped with a full VR setup — <span style="background-color: #abc4c5; padding: 2px 4px;">headset and full-body tracking system, including trackers on his legs and hips</span>. </p>
                        <p>Hunting’s film was not the only example of a documentary entirely shot in a virtual environment. During the pandemic, two unemployed British actors — Sam Crane and Mark Oosterveen — attempted to stage <i>Hamlet</i> inside the online version of the game <i>GTA</i> and documented the attempt in the film <i>Grand Theft Hamlet</i> (2024). They chose an amphitheater in the game, Vinewood Bowl, as the location for the performance. Finding actors for the roles, however, proved challenging, as the GTA player community is more interested in violence and chaos than classical drama (some lines were ultimately drowned out by gunfire). Like in <i>We Met in Virtual Reality</i>, the digital world of <i>GTA Hamlet</i> is populated with avatars that have human voices (players use headsets to exchange lines, just as in VRChat) and filled with human stories: during casting, one participant shared that she had always dreamed of being an actress, and now, finally, she had the opportunity.</p>
                        <p>Machinima (from "machine" and "cinema") is a long-established form of in-game fan fiction. The Janitor, the first feature-length machinima film within GTA, was made back in 2010 by a user named Mathieu Weshler. It tells the story of a New York janitor leading a double life — in addition to his official job, he acts as a vigilante, cleaning the streets not only of trash but also of criminals. However, <i>Hamlet</i>, which premiered at the major documentary film festival CPH:DOX and was digitally released on the cinephile platform Mubi, was probably the first documentary machinima to enter the hierarchical context of cinematic art. Even after losing contact with the physical world and real objects, documentary machinima remains closely connected to the lives of real human beings, even if they appear as non-human creatures. VRChat and GTA prove to be as inhabitable spaces for human existence as physical locations. Authenticity is no longer determined by technical recording characteristics or the indexical relationship of the image to objects, but by the sincerity of the experience it conveys — Rithy Pan’s clay figurines or the flurry of avatars in VRChat can be no less informative and “truthful” than traditional cinematic footage.</p>

            </div>
        </section>

        <!-- CHAPTER 9 -->
        <section id="chapter9" class="chapter">
            <h2 class="chapter-title">Chapter 9. The Eye of God. Jon Rafman as Prophet of the New Visuality</h2>
            <div class="chapter-content">
                <p>In March 1995, at a press conference in Paris marking the end of cinema’s first century, Danish director Lars von Trier unexpectedly began handing out brochures containing the manifesto of the newly formed <i>Dogme 95</i> movement, which had just emerged in Copenhagen. The manifesto called on filmmakers to abandon artifice and return cinema to its essentials — low-budget production and raw imagery. The text laid out a set of strict rules, a <i>Vow of Chastity</i>, which required shooting on location without additional props, using natural light and handheld cameras — regardless of whether the camera’s shake might be too noticeable on screen. This approach was meant to bring viewers closer to the cinematic experience, make them part of the action, and emphasize the authenticity of the emotions portrayed.</p>
                <p>By 1998, the first two films made according to the manifesto’s rules — <i>Festen</i> by Thomas Vinterberg and <i>The Idiots</i> by Lars von Trier — were competing at the Cannes Film Festival, turning <i>Dogme</i> into one of the most intriguing and famous phenomena in auteur cinema. Although the <i>Vow of Chastity</i> included ten points, <i>Dogme 95</i> quickly became primarily associated with the shaky handheld camera. To understand how unusual this kind of imagery seemed on the big screen at the time, it’s worth recalling the numerous reports of dizziness and nausea during screenings of <i>The Blair Witch Project</i> (1999) — a phenomenally successful low-budget post-<i>Dogme</i> horror film mimicking home video — and <i>Dancer in the Dark</i> (2000), where Trier had begun to move away from the strictures of the <i>Vow of Chastity</i> but had not yet fully abandoned its asserted aesthetic. Soon, the shaky camera made its way into Hollywood: in 2002, <i>The Bourne Identity</i> by Doug Liman was released, becoming a major commercial success and turning Matt Damon into one of the highest-paid actors in history. A kind of anti-Bond, this film and its sequels popularized the use of handheld cameras and a more immediate, documentary-style approach in commercial cinema, which inevitably influenced the Bond franchise itself, which over time also had to abandon its previously static, glamorous imagery.</p>
                <p>“The sequence becomes a jagged collage of fragments of explosions, crashes, physical lunges, and violently accelerated motions. There is no sense of spatiotemporal continuity; all that matters is delivering a continual series of shocks to the audience.” writes Steven Shaviro in the introduction to the comprehensive anthology Post-Cinema, pointing to <i>Dogme</i> directors as creators of a “post-continuum style” and quoting critic David Bordwell: “Hollywood action scenes became “impressionistic,” rendering a combat or pursuit as a blurred confusion.” Digital culture researcher Matthias Stork devoted his influential video essay  <i>Chaos Cinema</i> to this phenomenon, identifying its main symptoms: the abandonment of the visual clarity characteristic of 20th-century action films in favor of sensory overload, jagged editing, close-ups, and excessive camera movement. One example  Stork cites is the car chase in Quantum of Solace (2008), the second James Bond film released after the success of the Jason Bourne franchise. He notes that the visual confusion is compensated for by sound — and there is a purely commercial explanation for this phenomenon: Dolby technology in the 1990s became a key factor in bringing audiences back to cinemas after the disruption caused to the medium by television and domestic viewing.</p>
                <p>The “invention” of a new visual language by Trier and his peers, as Shaviro describes it, feels too close to the old “auteur theory,” and I find it hard to agree with. Trier and his colleagues did not invent the shaky camera that once made early viewers nauseous — just as, a century earlier, <i>Arrival of a Train</i> had sent the first Lumière audiences into panic. But they were among the first to sense a shift in the relationship between viewer and image, brought about by the mass spread of home video cameras and VCRs at the end of the 20th century. Until then, moving images were seen mainly on television and in movie theaters, with the cinematic image — produced by teams of professionals — ranked higher than the “cheaper” television picture. But by the end of the century, humanity was inundated by a flood of stumbling, gasping video fragments, roughly shot but emotionally charged, capturing both meaningful and everyday moments of life. With the advent of digital cameras and open internet platforms, that flood became an endless ocean. The experience of viewing and producing video changed the implicit contract between viewer and image, expanding the boundaries of what was acceptable on screen; the authors of the <i>Dogme Manifesto</i> already sensed this in the mid-1990s. What audiences would have considered a technical flaw just a few decades earlier became the new normal — something the eye now instinctively recognizes as a valid representation of reality. As has been noted throughout this book, every new technology eventually permeates culture and becomes normative — and more than that, it can profoundly affect the social, psychological, axiological, and even biological logic through which we live our daily lives. With the rise of AI-generated imagery, we are witnessing yet another shift in how reality is perceived. These hallucinatory images — provoking both fascination and rejection — cannot help but alter our sense of what an image can be. They inevitably lead to the emergence of a new agreement between screen and viewer, to the expansion of what is considered visually and conceptually permissible.</p>
                  <p></p>
                <p>***</p>
                <p></p>
                <p>One of the artists who, like the members of the <i>Dogme</i> movement, was among the first to sense the coming shift was the Canadian artist Jon Rafman, whose work explores the complex interplay between technology, reality, and human experience. “I’m trying to understand the profound transformations and changes that are occurring and that are often imperceptible because either they are too ubiquitous or we don’t have categories yet to make sense of them.” he said in an <a href="https://www.bernheimgallery.com/usr/documents/press/download_url/57/portrait-jon-rafman-spike-art-magazine.pdf" target="_blank">interview</a>, describing his artistic credo.</p>
                <p>In 2007, Google launched Street View — a constantly updated panoramic map of streets and roads captured by slowly moving cars, motorcycles, camels, or even people on foot, eventually covering around a hundred countries. Just a year later, in 2008, Jon Rafman introduced his project Nine Eyes of Google Street View (or <i><a href="https://9-eyes.com/" target="_blank">9-Eyes</a></i>), a collection of unique images that accidentally appeared online as a byproduct of the tech giant’s megalomaniac mapping initiative. The project, still accessible to the public on its dedicated website, was named after the Google Street View camera rig, which uses nine lenses to create 360° panoramic images. Exploring the overlooked corners of the digital projection of the world — places that might otherwise never have been seen — Rafman assembled a visual archive of strangeness, reflecting the vast range of human experience and the surrounding environment. Car crashes, crime scenes, obscene gestures, carnival costumes, a rearing horse, children brandishing weapons. Figures are distorted by the motion of the Google car; faces are blurred “for ethical reasons,” as if mocking the very idea of privacy. The context of each scene is impossible to reconstruct — one can only imagine it. The spaces are ordinary, even ugly, the kind no one would ever choose to photograph as a memory. The images are at once comic and bleak, like stills from an unscripted Harmony Korine film. Seen today, in a world saturated with AI-generated images, they look like the output of a free Stable Diffusion model — and yet, they are real.</p> 
                <div class="note">
   After the Dogme manifesto, Harmony Korine — now a video game developer — published the Mistakist Manifesto, which contained only three rules: “1. No plots. Only images. Stories are fine. 2. All editing effects must be done in-camera. 3. 600 cameras / wall of images / Phil Spector cinema.” The last point refers to the recording technique known as the “wall of sound,” invented by the innovative music producer Phil Spector. <span style="background-color: #abc4c5; padding: 2px 4px;">This approach involves layering multiple instruments and sounds to create a dense, powerful sonic texture, which had a major influence on musical and audiovisual aesthetics</span>.

</div>
                <p>Smartphone cameras have gradually trained us to see digital photography as an unpredictable mix of the ordinary and the extraordinary — after all, we usually reach for our phones either to capture everyday reality or to document moments of extremity. But <i>9-Eyes</i> represents the ultimate encounter between the human gaze and the digital image — none of these scenes were meant to be photographed; they simply happened to fall within the path of a passing camera car. No one else would ever have made them visible to the world except an artist who came of age at the intersection of analog and digital media, reclaiming these anonymous digital excesses as his own works.</p>
                      <p></p>
                <p>***</p>
                <p></p>
                <p>A representative of the first generation of gamers (he was born in 1981), a film enthusiast, and a follower of Chris Marker in his exploration of the nature of images, Rafman consistently examines how technology shapes our perception of reality. Among his other works is <i>Dream Journal</i>, a series of DIY 3D animations created between 2016 and 2019. Later, they were edited into a feature-length film that explores how a fusion of mythological fragments (creatures with bull heads, mermaids, images of hell) and modern media artifacts influences the subconscious.</p>
                <p>Rafman’s interest in the impact of technology on humans brought him together with Balenciaga’s creative director Demna Gvasalia. They met at Art Basel, and their collaboration resulted in an installation shaped like a futuristic tunnel that replaced the traditional runway for the Spring/Summer 2019 show. The tunnel’s surfaces were turned into screens, taking the audience on a journey through visual mutations that were both mesmerizing and unsettling — sometimes resembling digital noise, sometimes evoking the archaic layers of internet culture. The commercial for the collection, directed by Rafman, is a DIY-style action short that uses outdated digital effects and playfully parodies The <i>Matrix</i>.</p>
                <p>In an earlier project, <i>Brand New Paint Job</i> (2013), the artist reinterpreted traditional forms of art within a contemporary digital environment. Using the 3D Warehouse service integrated with Google’s software for architectural and 3D design, he applied textures borrowed from the works of well-known artists — Jackson Pollock, Jean-Michel Basquiat, Yves Klein, Marc Chagall, and others — onto realistic interior models. The project sparked protests from the art community, much like today’s criticism of using artists’ work to train AI models without their consent. (Unfortunately, this was not the only controversy in Rafman’s career: in 2020, three women accused him of inappropriate sexual behavior, which led to the cancellation of several of his exhibitions.)</p>
                <p>One of his works from the early 2020s was <i>Counterfeit Poast</i> (2022), a series of eerie post-human figures — a brown blot with multiple eyes, a face made of colored clay, a split seal, a symbiont combining a child and a puppy, and a pair of mutant lovers set against the cloud of a nuclear explosion. The grotesque, according to the artist, lies at the intersection of eroticism and death and possesses a distinct sensual quality. The images for the project were created using text-to-image AI models — several months before the technology became widely available and entered the practice of many artists. For Rafman, the arrival of artificial intelligence became another reason to explore questions of falsification and authenticity in visual content, its impact on human perception, and how it shapes our understanding of what is fake and what is real.</p>

                <p><i>Counterfeit Poast</i> largely continues the themes introduced in <i>9-Eyes</i>, yet it is this earlier work by Rafman that remains his most well-known, inspiring countless imitations in popular culture — including TikTok videos about exes and the dead found on Google Street View.</p>
                <p>It is in the context of <i>9-Eyes</i> that Rafman’s music videos for Kanye West and James Blake, created with generative AI, should be viewed. Today, as we have seen, AI experts seek maximum realism in video generation, while uninitiated viewers recoil from the creations of the uncanny valley. But in Vultures, Rafman uses Midjourney and the commercial video generator Runway Gen-2 to transport the viewer into a borderland where ghosts seem equally at home in the subconscious and on Google Street View. For an eye already accustomed to the strange, this video — populated by clowns, dogs, and car crash victims — may even appear almost documentary.</p>
                   <p></p>
                <p>***</p>
                <p></p>
                <p>The borderline state of a world captured and perceived through digital images becomes even sharper in James Blake’s new video <a href="https://youtu.be/sglbIVO9IQQ?si=wE0lGk4tYUo4_cpu" target="_blank"><i>Like The End</i></a>, released shortly after Donald Trump was elected president for the second time. Set to Blake’s melancholic music, strange visions drift past us for five minutes, embodying the fears and anxieties of the present moment. These images resemble a family photo album irradiated by an invisible nuclear blast — everything seems to have remained the same, yet has been radically and dramatically transformed. A baby playing on half a dozen monitors. Blake’s lyrics spelled out on the floor with crabs, written across the face of a meditating man, on the stomachs of young women in bikinis. Creepy laughing mannequins, an alligator in a drainage ditch, Teletubby-like figures with rifles relaxing on a beach in VR headsets. We recognize what these frames are about — they pass like déjà vu — yet they have never existed before, and what they capture is impossible. “Would you come home if you didn't recognize it?” asks Blake. Could you return home if it no longer looked like home? We already know the answer — home, as we once understood it, no longer exists.
                </p><p>Rafman believes that the distinction between virtual and real experience has become outdated in today’s world, and Baudrillard’s idea that we live in a state of hyperreality is no longer just a theoretical abstraction.</p>
                <p>Rafman isn’t the only one who senses this shift. Returning to cinema, we can recall Andrea Arnold’s 2024 film <i>Bird</i>. A direct heir to British realism and <i>Dogme 95</i>, Arnold has always been highly attuned to the visual language of her time and to the visual in general. Her feature debut <i>Red Road</i>, which won the Jury Prize at Cannes in 2006, told the story of a woman observing the world through surveillance cameras in Glasgow — every one of Arnold’s films, in some way, deals with the act of watching. Yet despite her reputation as a realist, in <i>Bird</i> Arnold presents a world where everyday reality and hallucination merge, and part of the narrative unfolds on a smartphone screen through which the teenage heroine records strange images that resemble those generated by AI or found on Google Street View. This is what the representation of reality looks like today.</p>
                <p>A world where it’s no longer just impossible to tell the fake from the real — where the very question itself becomes irrelevant — feels unsettling and even frightening. And if the <i>Dogme</i> filmmakers, just before the rise of digital cinema, tried to equate the image with raw reality as a way to hold back its mutations, today we can see that such mutations are unstoppable, involving every pixel of every image ever created. The eerie digital hybrids that emerge through generative processes have already entered our reality — perhaps they are us.</p>
                
            </div>
        </section>

        <!-- ACKNOWLEDGMENTS -->
        <section id="acknowledgments" class="chapter">
             <h2 class="chapter-title">Disclaimer</h2>
            <div class="chapter-content">
                <p>This book was written with the assistance of LLM services — Claude, Perplexity, and ChatGPT. Fragments of generated responses included in the text without modification are <span style="background-color: #abc4c5; padding: 2px 4px;">highlighted in color</span>. The chapters <i>Anti-Deleuze: Classical Film Theory Breaks Under the Influence of AI Without Losing Its Explanatory Power</i> were co-written with the Claude AI.
</p>
          
            </div>
            <h2 class="chapter-title" style="margin-top: 80px;">Acknowledgments</h2>
            <div class="chapter-content">
                <p>This book would not have been possible without the experience of studying in the Art & Science master’s program at ITMO University, and especially without the inspiring seminars of artist <a href="https://nikonole.com/" target="_blank">Helena Nikonole</a>, head of the <i>Digital Worlds</i> studio. Among other faculty and staff at the Art & Science Center who encouraged me to dive into this subject were the duo <a href="https://graycake.com/ru/" target="_blank">Gray Cake</a> (Alexander Serechenko, Ekaterina Pryanichnikova), Nadia Bey, Ksenia Gorlanova, Ellina Gennadyevna, and Anna Kaplan. <p></p>I would also like to thank Tatyana Shorokhova and all my co-authors at kkbbd.com, as well as Kantemir Balagov and Kira Kovalenko, without whom I wouldn’t have had a selfie with Kristen Stewart.</p>
            </div>

        </section>

        
            </div>
        </section>

        <!-- BIBLIOGRAPHY -->
<section id="bibliography" class="chapter">
   
    <div class="chapter-content">
         <h2 class="chapter-title">Bibliography</h2>
        <h3>Books</h3>
        <ul>
            <li>Бойм С. <em>Будущее ностальгии</em>. М.: Новое литературное обозрение (2001, 2021).</li>
            <li>Манович Л. <em>Теории софт-культуры</em>. Н.Новгород: «Красная ласточка» (2017).</li>
            <li>Кроуфорд К. <em>Атлас искусственного интеллекта</em>. М.: АСТ (2023).</li>
            <li>Gerovitch S. <em>From Newspeak to Cyberspeak: A History of Soviet Cybernetics</em>. Cambridge: The MIT Press (2004).</li>
            <li>Пруденко Я. <em>Кибернетика в гуманитарных науках и искусстве в СССР. Анализ больших баз данных и компьютерное творчество</em>. М.: Музей современного искусства «Гараж» (2018).</li>
            <li>Hauskeller М., Philbeck T.D., Carbonell C.D. (eds). <em>The Palgrave Handbook of Posthumanism in Film and Television</em>. London: Palgrave Macmillan (2015).</li>
            <li>Гомбрих Э., Блэк М., Хохберг Дж. <em>Искусство, восприятие и реальность</em>. М.: Ad Marginem (2023).</li>
            <li>Denson Sh., Leyda J. (eds). <em>Post-Cinema: Theorizing 21st-Century Film</em>. Falmer: REFRAME Books (2016; 2024).</li>
            <li>Shobeiri A., Westgeest H. (eds). <em>Virtual Photography. Artificial Intelligence, In-game, and Extended Reality</em>. Bielefeld: transcript publishing (2025).</li>
            <li>Shaviro S. <em>Post-Cinematic Affect</em>. London: Zero Books (2010).</li>
            <li>Scolari C.A. <em>On the Evolution of Media. Understanding Media Change</em>. Abingdon: Routledge (2024).</li>
            <li>Эльзессер Т., Хагенер М. <em>Теория кино. Глаз, эмоции, тело</em>. С-Петербург: Сеанс (2017, 2024).</li>
            <li>McDonald K. <em>Film Theory: The Basics</em>. Abingdon: Routledge (2022).</li>
            <li>Хобсбаум Э. <em>Эпоха крайностей. Короткий двадцатый век. 1914-1991</em>. М.: Издательство Независимая газета (2004). Стр. 546.</li>
            <li>Nussbaum E. <em>I Like to Watch: Arguing My Way Through the TV Revolution</em>. N.Y.: Random House (2019).</li>
            <li>Делёз Ж. <em>Кино</em>. М.: Ad Marginem (2019).</li>
            <li>Zylinska J. <em>The Perception Machine: Our Photographic Future between the Eye and AI</em>. Cambridge: The MIT Press (2023).</li>
        </ul>

        <h3>Articles</h3>
        <ul>
            <li>Кувшинова М. Тамагочи не плачут; Кто такая Элис // Известия (3 ноября 2001)</li>
            <li>Кувшинова М. Помешательство в обезьяннике // Известия (31 марта 2001)</li>
            <li>Столбова Н., Середкина Е., Мышкин О. <a href="https://cyberleninka.ru/article/n/naskolko-zloveschaya-dolina-zlovescha-na-samom-dele-opyt-dekonstruktsii-diskursa/viewer" target="_blank">Насколько “Зловещая долина” зловеща на самом деле? Опыт деконструкции дискурса</a> // Вестник Пермского университета (Выпуск 1, 2022)</li>
            <li>AI-Generated text edited by Gloria Maria Cappelletti. <a href="https://red-eye.world/c/ai-art-and-the-collective-subconscious-exploring-new-frontiers-with-red-eye-s-radar" target="_blank">AI Art and the Collective Subconscious: Exploring New Frontiers with RED-EYE's RADAR</a></li>
            <li>Gogol N. <a href="https://www.youtube.com/watch?v=gyI2hgt-xpo&list=LL&index=107" target="_blank">The Nose</a></li>
            <li>Kätsyri J. and others. <a href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2015.00390/full" target="_blank">A review of empirical evidence on different uncanny valley hypotheses: support for perceptual mismatch as one road to the valley of eeriness</a> // Frontiers in Psychology (2015)</li>
            <li>Зуйков И. <a href="https://cyberleninka.ru/article/n/tsifrovoy-kinematograf-v-rakurse-effekta-zloveschey-doliny/viewer" target="_blank">Цифровой кинематограф в ракурсе «эффекта зловещей долины»</a> // Вестник ВГИК. Том 13, N4(50) (декабрь, 2021)</li>
            <li>Berryman R., Abidin C., Kim D. O. D., Hwang, S., Miyake, E. <a href="https://doi.org/10.5210/spir.v2024i0.14096" target="_blank">VIRTUAL CELEBRITY INDUSTRIES IN EAST ASIA</a> // AoIR Selected Papers of Internet Research (2025)</li>
            <li>Van Es К. <a href="https://www.researchgate.net/publication/363867276_Netflix_Big_Data_The_Strategic_Ambivalence_of_an_Entertainment_Company" target="_blank">Netflix & Big Data: The Strategic Ambivalence of an Entertainment Company</a> // Sage Journals. Television and New Media, Volume 24 Issue 6 (September 2023)</li>
            <li>Khamsi R. <a href="https://www.nature.com/news/2005/050620/full/news050620-7.html" target="_blank">Jennifer Aniston strikes a nerve</a></li>
            <li>Stork M. <a href="https://pressplayredux.com/2011/12/09/video-essay-chaos-cinema-part-3-matthias-stork-addresses-his-critics/" target="_blank">Chaos Cinema</a></li>
            <li>Ulenius M. <a href="https://www.diva-portal.org/smash/get/diva2:1601121/FULLTEXT01.pdf" target="_blank">Tall Tales Ancestry and Artistry of Vertical Video</a></li>
            <li>Ross M., Glen M. <a href="http://www.rhizomes.net/issue26/ross_glen.html" target="_blank">Vertical Cinema: New Digital Possibilities</a></li>
            <li>Schofield B. <a href="https://airmail.news/issues/2025-7-19/cliff-hangers-for-the-commute" target="_blank">Cliff-Hangers for the Commute: One-minute micro-dramas</a></li>
            <li>Atherton N. <a href="https://amateurphotographer.com/latest/opinion/why-i-hate-vertical-video/" target="_blank">Why I hate vertical video</a></li>
            <li>Gomez J., Barnett M., Grill-Spector K. <a href="https://www.nature.com/articles/s41562-019-0592-8" target="_blank">Extensive childhood experience with Pokémon suggests eccentricity drives organization of visual cortex</a> // Nat Hum Behav, Jun 3(6) (2019)</li>
            <li><a href="https://www.youtube.com/watch?v=p6fp2oZI2OI&t=35s" target="_blank">VR CHAT - Пятерочка </a></li>
        </ul>
    </div>
</section>

    </div>

    <script>
        // Плавная прокрутка к якорям
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {
                    target.scrollIntoView({
                        behavior: 'smooth',
                        block: 'start'
                    });
                }
            });
        });
    </script>
  <div style="text-align: center; font-size: 12px; color: #999999; padding: 40px 20px;">
    © Maria Kuvshinova (2025)
</div>
</body>
</html>